###############################################################################################################################################
################################### general question ####################################################################################### 
#########################################################################################################################################

1.can you please brief me about yourself ?
Hi Mam/Sir
I have total 3 YOE  working in Cloud and  DevOps domain. Currently im associated with organization called prevoyance IT solution.
Talking about my roles & responsibility - my work revolves around providing support to multiple projects by using various AWS Services like ec2 VPC cloud front cloud trail s3 cloud watch auto scaling load balancer IAM EKS trusted advioser and many more services like this. At the same time i need to convert all manual activities into automotive way by using diff devops tools like for deploying microservices using  docker & Kubernetes. terraform for infrastructure creation, Jenkins for CI/CD  pipeline and for monitoring purpose using Prometheus, Grafana.



2.So tell me like how your work is carried out on daily basis ?
usually we need to attend one scrum call in morning, in that call we need to update on the given task and we discuss the issues that we are facing in our task, then i go thrugh my mails and i will revert on the mail if it is required from my side. then i check the jira tickets status if there are any pending task in my tray or any new task assigned to me. then i usually monitor all the dashboards on daily bases.   

3.how tickets are created in your company ? do you know any ticketing tools ?
tickets are created by product owner  and assigned to us, and the ticketing tool whcih we use is jira. 

4. how you will design 3 tier architecture by considering few points like High availability , fault tolerance  

To build 3 tier architecture, firstly i will make sure my network setup is ready which includes vpc,subnets and routes configuration.
Then i will have my front end, backend and storage deployed. Front end would be my load balancer ,which will run in public facing .backend would be ec2 in pvt subnet and storage can be either database in pvt subnet or s3.
Just to make sure there is  high availability and fault tolerance i will make sure ec2 are launched in multiple pvt subnets. I will do autoscaling for the same.
Once my setup is ready then i will use some  services to monitor and keep env secure like cloudwatch,cloudtrail,guard duty,CDN,WAF,Trusted advisor etc

5. when we say  High availability , so what exactly it means  ?
High Availability, or HA, means designing a system so that it remains operational and accessible even if some of its components fail. The goal is to minimize downtime and ensure continuous service availability.

In simple terms, it’s about eliminating single points of failure and building redundancy at every layer — such as using multiple servers, load balancers, and replicated databases.

For example, in AWS we can achieve high availability by deploying applications across multiple Availability Zones (AZs). If one AZ goes down, the application still runs from another AZ without impacting users.

6. full form of MTTR MTTD RTO RPO 

Mean Time to Recovery/Restore: The average time it takes to restore a system or service after an outage begins. It is a key metric in a DevOps context.

Mean Time to Detect: The average time that elapses between the start of a system failure, security incident, or performance issue and the moment it is discovered by the organization. A low MTTD indicates an effective monitoring system. 

Recovery Time Objective: The maximum acceptable amount of time that a system, application, or business process can be offline following an unexpected disruption. It defines the target for restoring operations to avoid unacceptable consequences. 

Recovery Point Objective: The maximum acceptable amount of data loss that an organization can tolerate during an incident, measured in time. It determines how frequently data must be backed up to meet this objective. For example, an RPO of 1 hour means you can only afford to lose up to 60 minutes of data. 

7. what are the daily task activity that you use to do in your job role. 
daily i use to work on major and minor updates of terraform modules. 
deploying and troubleshooting application on EKS cluster. 
atomating the AMI backup and configuaring aws cloud watch.  



\*\*\*EC2

1. what is mean by cloud computing ?

Ans: Cloud computing is on demand delivery of IT resources over internet with pay as you go with pricing. Instead of using physical servers and data centers, you can use services like computing power, storage, database, networking on the basis of need from cloud service providers.



2. what are the different cloud service providers you know ? On which cloud platform you have worked ?

Ans: There are different cloud service providers like AWS (Amazon Web Service), GCP (Google Cloud Platform), Microsoft azure, Alibaba cloud, Oracle cloud, IBM cloud.

          I have worked on AWS.



3.which type of instances ( like Linux or windows) you have created in your company ?

Ans: I have created instances like Linux and windows. In Linux i have created ubuntu, centos, Amazon Linux.

\+

4.So In your current company what process do you follow  "BEFORE" creation of instance ? Do you have some conversation with clients  ?

Ans: Before creating an instance for client, we always having conversation with client. We understand their requirement of AMI (Amazon Machine Image), Instance Type, Number of instances, Storage size, Storage type, IOPS.



5.What are  the steps you have followed while creation of instances ? Explain each step in brief..

Ans: Steps for creating an instances are

           i) Add Tags: This step is used to tag instances and volumes.

           i) Choose AMI : This step is used to select an operating system of instance.

          ii) Choose Instance Type: This step is used to select CPU and RAM for instance.

         iii) Configure Instance: network and subnet.

         iv) Add Storage: This step is used to attach additional EBS volume and Instance store volume.

        vi) Configure Security Group: This step is used to create set of firewall rules that control traffic for instance.

       vii) Review instance launch: This step is used to review above steps.



6.what is the difference between server creation steps of Linux and windows machine ?

Ans:  To create Linux machine Linux AMI is used. To create windows machine windows AMI is used.

           To create Linux machine SSH firewall rule is required. To create windows machine RDP firewall rule is required.

           To create Linux machine .ppk file is required. To create windows machine .pem file is required.



7.which instance type you have used in your organization to create the instances ? on what basis you use to select the instance type ?

Ans: Instance type are

          i) Compute Optimized instance type: These are high performance processors. eg: c series i.e. c4.large, c4.xlarge, c4.2xlarge, c4.4xlarge, c5.xlarge, c5.2xlarge

         ii) Memory Optimized instance type: These instances are used to process large amount of data in memory during workload. eg: R5 series i.e. r5.large, r5.xlarge, r5.2xlarge, r5.4xlarge

        iii) Storage Optimized instance type: These instances are used for high, sequential read and write access of data in storage. eg: i series i.e. i.large, i.xlarge, i.2xlarge, i4xlarge
 
        iv) General Purpose instance type: In these instance type compute resource, memory resource, network resource are balanced. eg: T series i.e. t2.small, t2.medium, t2.large, t2.xlarge, t2.2xlarge, Mac series

 

        In my organization i have used instance type as per client requirement. Mostly i have worked on compute optimized and memory optimized+ instance type because our project focus was to use high performance processors and high memory.



8. Port no for SSH and RDP ?

Ans: Port no. for SSH is 22

          Port no. for RDP is 3389



9. what is the use of Tagging ? how it was helpful for you in your company ?

Ans: Tagging is used to search particular instance from a list of instances.

          In my company there were 90-100 instances. if i was wanted to make any changes in particular instance having some name. Then due to tagging of instances it was easy to search and get that particular instance from list of instances.

 

10. why do we need security group ( S.G ) ?

Ans: Security group acts as a firewall at instance level. It is needed to control the traffic on the instance.

Security group is stateful. It means that we can only allow traffic and we cannot deny traffic in that.



11.can we create only security group ? or is it mandatory to create S.G while server creation only ?

Ans: Yes, we can create only security group and use then while creating instances. it is not mandatory to create security group while server creation only.



12.what is default traffic allowed when you create  S.G ?

Ans: when we create Security group, outbound rules allows All traffic and inbound rules will contain traffic which we will allow for the instance.



13.what are the different volume types ? based on what you use to select the volume type in your company ?

Ans: Different types of volume

        i) General Purpose

       ii) Provisioned IOPS

      iii) Magnetic



     In our company we select Volume type based on requirement of IOPS. IOPS means Input Output Per Second. In our company i have used general purpose volume type because our project was not required large number of IOPS.



     i) General Purpose Volume are used in:

        Transactional workload, Virtual desktops, Medium size single instance database, Low latency interactive applications, Boot volumes, Development and test environment.

    ii) Provisioned IOPS volume are used in:

        Sub millisecond latency, sustained IOPS Performance, more than 64,000 IOPS

   iii) Magnetic Volumes are used in:

        Magnetic volumes are used in workload where data is accessed infrequently.



14.What does status check (3\*3) means ?? how you will troubleshoot or remove that status check failed alert  ?

Ans: Status check means monitor AWS system on which our instance runs.

          There are two types of status check

           i) System status check : it means that issue is from AWS. Then to solve this issue i will connect to AWS support.

          ii) Instance status check : It means that issue will be from our side. i need to check my network configuration like vpc , subnets are configured properly or not.

          iii) Reachability Check (new in some views) – Confirms that the instance responds to AWS network probes (ping/SSH/HTTP reachability).

 

       \*\*For troubleshot or to remove that error  :- i will stop and start my instance.


15.Can we take Backup of EC2 machines ? give me few examples where  it was needed to take backup ?

Ans: Yes we can take backup of EC2 machine through AMI.

           we need to take backup of instance when

           - customer don't require that instance but they want their data and backup for future use.

           - when customer want to create more instances with same configuration and data.

           - To create copy of data and configuration that can be recovered in the event of server crash.



16. how will you take complete backup ? what is online and offline backup ??

Ans: Steps to take backup

          i) Select instance whose backup you want to take

         ii) Go to Actions --> Image and templates --> Create image

        iii) inside Create image give image name. select No reboot option. and click on Create image option to create an image.



          \*\* Online backup means backup taken when instance is in running state.

          \*\* Offline backup means backup taken when instance is in stop state.



17. Suppose there is server-A and you want 4 more servers with same configurations and data then how will you do it ?

Ans: First i will take backup of server-A using AMI backup. Once status of AMI backup becomes available, i will select option "Launch instance". Here i will use that AMI backup and launch the ec2 machines.


18. Have you heard about User data in EC2 ? If yes what is the purpose of it and how you will add it ?

Ans: AWS user data is a set of commands you can provide to a instance at the time of launching.  I will not require to login into EC2 and execute the commands separately.

Example: Suppose i want to install Apache application in EC2, then using user data, i will simply add commands to install Apache application into EC2 during EC2 creation.

 

19. to avoid disaster scenarios,  what would be the best practice of creating the ec2 machines ?

Ans: Following would be the best practice of creating the ec2 machines

          1. Security:

            i) Manage access to AWS resources and APIs using identity federation, IAM roles, IAM users.

           ii) Establish credentials management policies and procedures for creating, distributing, rotating and revoking AWS access credentials.

          iii) Implement least permissive rules for your security group.

          iv) Regularly patch, update and secure the operating system and applications on your instance.

          v)  Use Amazon inspector to automatically scan EC2 instances for software vulnerabilities.
         2. Storage:

           i) Understand the implications of root device type for data persistence, backup and recovery.

          ii) Use separate Amazon EBS volume for the operating system versus your data. Ensure that volume with your data persist after instance termination.

         iii) Use instance store available for your instance to store temporary data. Remember that data stored in instance store is deleted when you stop, hibernate or terminate your instance.

         iv) Encrypt EBS volumes and snapshots.



        3. Resource management:

          i) Use instance metadata and custom resource tags to track and identity your AWS resources.

         ii) View your current limits for Amazon EC2. Plan to request for limit increase in advance of time that you will need.

        iii) Use AWS Trusted Advisor to inspect your AWS environment, and then make recommendations when opportunities exist to save money, improve system availability and performance.

 

       4. Backup and Recovery:

        i) Regularly back up your EBS volumes using Amazon EBS snapshots, and create an Amazon Machine Image (AMI) from your instance to save configuration for launching future instances.

       ii) Deploy critical components of your application across multiple Availability zones and replicate your data appropriately.

      iii) Design your applications to handle dynamic IP addressing when your instance restarts.

      iv) Monitor and respond to events.

       v) Regularly test the process of recovering your instances and Amazon EBS volumes to ensure data and services are restored successfully.

 

      5. Networking:

       i) Using appropriate subnets like private or public to launch the servers. Using NACL and security groups, vpc flow logs.



20. How you will optimize the cost of ec2 machine ?

Ans: Following are some ways to optimize the cost of ec2 machine.

       i) Right size your EC2s.

      ii) Use tags to target your cost-optimization.

     iii) Stop non-production instances when they're not in use.

     iv) Scale Production Instances in Response to customer demand.

      v) Automatically terminate rogue EC2 instances before they cost you serious money.

     vi) Let AWS batch run a temporary fleet of instances when a job is ready.

    vii) Use spot pricing to secure temporary instances for less.

   viii) Use reserved instances to cut the cost of predictable instance use.

     ix) Replace instances with containers, functions and managed services.



21. Difference between On demand , reserved , dedicated and spot instance ?

Ans:

         On demand instances:

      i) With On-demand instances, you pay for compute capacity by second with no long term commitments.

      ii) you have full control over its lifecycle. You decide when to launch, stop, hibernate, start, reboot or terminate it.

     iii) There is no long-term commitment required when you purchase on-demand instances.

     iv) You pay only for the seconds that your on-demand instance are in running state,with a 60-second minimum. The price per second for a running on-demand instance is fixed.

     v) It is recommended that to use on-demand instances for applications with short-term, irregular workloads that can not be interrupted.

 

        Reserved instances:

     i) Reserved instances provide you significant savings on your Amazon EC2 cost compared to on-demand instance pricing.

    ii) These are not physical instances.

   iii) Reserved instance pricing is determined by instance type, region, tenancy, platform.



       Dedicated instances:

    i) Dedicated instances run in virtual private cloud (VPC) on hardware that is dedicated to single customer.

   ii) These type of instances that belong to different AWS accounts are physically isolated at hardware level, even if those accounts are linked to a single payer account.

  iii)  Dedicated instances may share hardware with other instances from same AWS account that are not dedicated instances.



       Spot instances:

    i) Spot instance is an instance that uses spare EC2 capacity that is available for less than on demand price.

   ii) The hourly price for a spot instance is called a spot price.

  iii) The spot price of each instance type in each availability zone is set by Amazon EC2. it is adjusted gradually based on long-term supply and demand for spot instances.

  iv) Spot instances ate cost-effective choice if you can be flexible about when your application run and if your application can be terminated.

  v) These instances are well suited for data analysis, batch jobs, background processing, optional tasks.



22..How you will login into server if you lost SSH keys ?

Ans: Let us assume that name of this instance is "Original Instance". and i will create one more linux instance and will give name "Temp Instance".

          I will give name "Original Volume" to Original Instance Volume and name "Temp Volume" to Temp Instance Volume.

          Both "Original Instance" and "Temp Instance" are having separate keypair. Suppose keypair of "Original Instance" is lost.

         To make login into "Original Instance" I will do following steps

          i) I will stop "Original Instance" and detach "Original Volume" from "Original Instance". Then i will attach "Original Volume" to "Temp Instance".

          ii) Now i will copy .ssh/authorized\_keys from "Temp Volume" to "Original Volume". Then detach "Original Volume" from "Temp Instance" and attach to "Original Instance".

         iii) Now i will try to login into "Original Instance" using keypair of "Temp Instance".

         iv) login into "Original Instance" using keypair of "Temp Instance" getting successfully.



23. can we attach multiple security group to one EC2 machine ?

Ans: Yes, we can attach multiple security group to one EC2 machine.



23. Can we attach one security group to multiple instances at the same time ?

Ans: No , we cannot same attach security group to multiple servers at the same time.



24. how you will increase the EBS volume size  attached to any ec2 machine ?

Ans: steps to increase the EBS volume size of ec2 instance.

       i) Select the ec2 instance whose volume size you want to increase.

      ii) Go to storage option of ec2-instance. Click on Volume ID.

     iii) Click on "Modify" option. Now enter the size of volume in size field. Then click on "Modify" option.

iv) once we click on modify, volume will go into optimization state. once the modification is completed then inside the server we need to execute linux command - growpart <file partition name>

25. how will you configure ec2 instance with RDS 
first create EC2 instance and RDS with my-sql, then collect the endpoint of RDS server, edit the inbound rule of RDS to security group to allow traffic from EC2 instance, and also configure the security group of EC2 instance for outbount rule, then install mysql client on ec2 instance configure the RDS using endpoint and its done. 

26. can we connect 2 vpc with the same load balancer 
yes, we need to do VPC pearinng for that. 

 

 

===================================================================================================================================================================

\*\*\*Load Balancer   ( kindly revise load balancer and ELB algorithms recordings before revising below questions.Make sure you understand the process ,then you can explain in your own words )



1\. what are the different types of load balancers ?

Ans: Types of load balancer are Application load balancer, Network load balancer, classic load balancer, gateway load balancer.

 

2\. Difference between Application and network load balancer ?

Ans:

         Application load balancer: it operates in 7th layer of OSI model.

         Network load balancer: it operates in 4th layer of OSI model.

         Application load balancer: Listeners are HTTP and HTTPS

         Network load balancer: Listeners are TCP, UDP, TLS

         Application load balancer: Target types are instance, IP and Lambda

         Network load balancer: Target types are instance, IP

         Application load balancer: it is used where quick access of application, website, database is required

         Network load balancer: it is used where excellent latency or very short response time is required.

 

3\. which type of load balancer you have used in your organization ?

Ans: In my organization, i have used Application load balancer and Network load balancer. There were some projects which was required quick access of applications, websites

          and databases. For this we were used Application load balancer. And some projects which were required excellent latency or low response time. For this we were used

          Network load balancer.



4\. steps to create application load balancer ?

Ans:

           Application Load Balancer:

          Step1: Configure Load Balancer: It includes to mention Name, scheme, IP address type. Select listener as HTTP and Availability zones.

          Step2: Configure Security Groups: Select security group for load balancer.

          Step3: Configure Routing: It includes to mention Target group, Health checks.

          Step4: Register Targets: Here target instances needs to be registered.

          Step5: Review is the step where details of load balancer is mentioned.

          Step6: Click on Create.



          Network Load Balancer:

          Step1: Configure Load Balancer: It includes to mention Name, scheme, select listener as TCP.

          Step2: Configure Routing: It includes to mention Target group, Health checks.

          Step3: Register Targets: select option private IP addresses. Use availability zone. Use IP address. And click on register.

          Step4: Review is the step where details of load balancer is mentioned.

          Step5: Click on create.



5\.  what does target group means in load balancer ?

Ans: Target group means for which servers we want to balance the load. In side target group we will have the set of servers on which traffic will be balanced..



6\. what is mean by listener in load balancer ?

Ans: Listener is a process which listens the traffic coming from outside and then transfer it to respective set of rules or target group.



7\. what are the different types of Algorithms used in Load balancer ? explain each of them ?

Ans: Load balancing algorithms

        i) Round Robin:

           Round robin algorithm is one of the simplest and most used load balancing algorithm. Client requests are distributed to application servers in rotation.

           for example, if there are three application servers then first client request to the first application server in the list. The second client request to the second application server.

           The third client request to the third application server, the fourth to the first application server and so on. This load balancing algorithm does not consider characteristics of

           the application servers. i.e. it assumes that all application servers are same with same availability, computing and load handling characteristics.



       ii) Sticky session:

           Sticky session is an algorithm that makes it so that a user will always be sent to the same server node. A server node contains all user-relevant information such as their activity on their website during their session.

           configuring sticky sessions can boost performance of application. it can keep data exchange to a minimum, by reducing number of times server need to exchange session data.

           Sticky sessions also increase server responsiveness by allowing the server to utilize their RAM cache more effectively.



       iii) Least connection:

          Least connection algorithm is a dynamic load balancing algorithm where client requests are distributed to the application server with least number os active connections at the time of client request received.

          In case where application servers have similar specifications, an application server may be overloaded due to longer lived connections. This algorithm takes the active connection load into consideration.



8\. why did you used load balancer in your project ? explain with one real time example ?

Ans: Load balancers improve application availability and responsiveness and prevent server overload. Each load balancer sits between client devices and backend servers, receiving and then distributing incoming requests to any available server capable of fulfilling them.

 

===================================================================================================================================================================

\*\*\* Auto scaling



1. what is the need of auto scaling ?

Ans: AWS auto scaling monitors our applications and automatically adjusts capacity to maintain steady, predictable performance at the lowest possible cost.

         Using AWS Auto Scaling it is easy to setup application scaling for multiple resources across multiple services in minutes.

         AWS auto scaling makes scaling simple with recommendations that allow you to optimize performance, costs.

        Benefits of auto scaling:

        i) Setup scaling quickly

       ii) Automatically maintain performance

      iii) make smart scaling decisions

      iv) pay only for what you need

 

2\. Based on which conditions you used to do auto scaling ?

Ans: In my company we use CPU but in general ,  Auto scaling is done based on four conditions

        i) Application Load balancer request count per target

       ii) Average CPU utilization

      iii) Average Network in (Bytes)

      iv) Average Network out (Bytes)



we have set threshold value for CPU, once that threshold value is crossed the instances scales up !!

 

4. what do we do in launch configuration step while configuring auto scaling ?

Ans: In launch configuration step we define which instance should be created when load will increase and auto scaling will be done.

          In launch configuration we select AMI (Amazon Machine Image), instance type, create launch configuration, Add storage, configure security group and review. then click on create launch configuration.

 

5. what are different scaling policies available ? explain with scenario ?

Ans:  Different scaling policies are

         i) Target tracking scaling policy

        ii)  simple scaling policy

       iii)  step scaling policy



        i) Target tracking scaling policy:

            - Here we set minimum and maximum value of instances.for example, we set minimum value = 2 and maximum value = 10.

            -  There are four metric type on which load is measured and autoscaling is done. These metrics are Application load balancer request count per target, Average CPU utilization, Average Network in, Average Network out.

            - Here Metric type as Average CPU utilization is selected. Target value is set as 70. it means that Average CPU utilization will try to maintain at 70. if it increases above 70 then it will increase instances and try to maintain Average CPU utilization at 70 and if it decreases below 70 then it will decrease instances and will try to maintain Average CPU utilization at 70.



       ii)  simple scaling policy:

            - Increase Group size is used to increase instances. Decrease Group size is used to decrease instances. create alarm is required for executing policy.

            - In Increase Group size create an alarm as cpu utilization >=70. it means that when cpu utilization will increase above 70 then it will increase instance. we can mention no. of instances to increase.

            - In Decrease Group size create an alarm as cpu utilization<=70. it means that when cpu utilization will decrease below 70 then it will decrease instance. we can mention no. of instances to decrease.

 

6. what is the use of stand by mode in auto scaling ?

Ans: Stand by mode means for sometime that server will not be considered for autoscaling. this is used when we are troubleshooting some issue on that server.

===================================================================================================================================================================

\*\*\*NETOWRKING  ( Revise class recording of mine to know the complete flow of VPC /public and pvt subnets /NACL/Bastion host/NAT)



1.What is VPC ( virtual private cloud)?

Ans: VPC means a logical boundary within which we launch our virtual machines and other resources. VPC helps us to build the environments with our own network

 

2.What is subnet ? types of subnet, what is public subnet and private subnets?

Ans: Subnet is groups of  range of IP addresses in our VPC. We can launch AWS resources like EC2 instances into a specific subnet.

 

\*\* Public subnet means which there is accessibility to go over the internet

\*\* Private Subnet means By default there is not accessibility to go over the internet



3.what is the use of  internet gateway ? what will happen if we do not attach internet gateway to VPC ?

Ans: Internet gateway is virtual router that connects VPC to the internet. If you create new VPC then you must attach internet gateway to VPC in order to access the internet.

          if you do not attach internet gateway to VPC then Public subnet is not access to the internet.

 

4.Explain the complete configuration / setup of networking ?

Ans: First create VPC --> create 2 subnet in same VPC --> create internet gateway and attach internet gateway to VPC --> create 2 route table --> in one route table we select the path via internet gateway that subnet is our public subnet -->

          create NAT Gateway in public subnet --> Make entry of this NAT Gateway in the route that is our private subnet.



\*\*\*5.Difference between Security group and NACL ?

Ans:



    \*Security Group \*                        				              \* NACL \*

\- Act as a firewall at instance level  					       - Acts as a firewall at subnet level

\- Security group is statefull (means we can only allow the traffic)            - NACL is stateless ( means we can allow and deny the traffic )

\- There is no such concept of Rules                                            - Priority is based on Rules. rule no1 will have high priority as compared to rule no 50



6.Suppose i want to communicate over the internet using the server which is launched in private subnet , then how will you do it ?

Ans: With the help of NAT GATEWAY.

          Instances in public subnet can send outbound traffic directly to the internet. but instances in private subnet can not send outbound traffic directly to the internet.

          Instances in private subnet can access internet by using NAT Gateway.

\*\* Kindly note - NAT gateway will be launch in public subnet.



7.How can we connect to server having only private IP ?

Ans: I will create on Bastion Host / Jump server in public subnet and from inside that server i will access the server having pvt ip only.



8.Is it possible to have communication between 2 servers which are launched in different VPC / Different Region or different account ? how ?

Ans: Allow traffic in Security group.

          Allow traffic in NACL.

          VPC peering in 2 VPC.



9.have u heard about bastion host / jump server ? what is it ? why you use it ?

Ans: Yes, with the help of bastion host / jump server i can access server`s running in  private subnet.

          Yes, we have configured bastion host in our company. we use bastion host to access ec2 instances which are running in private subnet. We launch bastion host in public subnet.

 

10.suppose there is one server in one vpc and other server in another VPC. How i can establish the connectivity between this 2 servers ?

Ans: Allow traffic in security group.

          Allow traffic in NACL.



11suppose there is one server in one vpc in North Virginia and other server in another VPC in Oregon region. How i can establish the connectivity between this 2 servers ?

Ans: Allow traffic in security group.

          Allow traffic in NACL.

         VPC peering in 2 VPC.

 



12.suppose there is one server in one vpc of AWS Account A and other server in another VPC of AWS Account B. How i can establish the connectivity between this 2 servers ?

Ans: Allow traffic in security group.

          Allow traffic in NACL.

         VPC peering in 2 VPC.



13. why  we need VPC Perring ? what is major condition we need to remember while doing peering ?

Ans: when we want to have communication between 2 different VPC then by default it is not possible. firstly we need to connect those VPC. this process is know as vpc peering.



Condition :  we need to make sure that vpc CIDR should NOT be same of both VPC otherwise we cannot established vpc peering.



11.OSI Model layer ?

Ans: OSI model means Open System Interconnection model. In OSI model there are 7 layers that is Application layer, Presentation layer, Session layer, Transport layer,

          Network layer, Data link layer, Physical layer.



12.Difference between TCP and UDP protocol ?

Ans: TCP - connection oriented protocol

          UDP - connectionless protocol

          TCP - reliable protocol as it provides assurance for delivery of data packets.

          UDP - unreliable protocol as it does not provide assurance for delivery of data packets.

          TCP - slower than UDP

          UDP - faster than TCP

          TCP - header size is 20 bytes

          UDP - header size is 8 bytes

          TCP -  non real time

           UDP - real time

          TCP - retransmission of package

          UDP - No retransmission of package

 

13.How you will establish communication in between server 1 and server 2 present in same subnets ?

Ans: By allowing traffic in security group.



14.How you will establish communication in between server 1 and server 2 present in two different subnets ?

Ans: By Allowing traffic in security group and allowing traffic in NACL.



15\. what are vpc flow logs ? why do we use them ? what is accept ok and reject in vpcflowlogs ?

Ans: VPC flow logs is a feature which help us to capture information about IP traffic going to and from network interfaces in VPC.

          Flow log data can be published to Amazon CloudWatch logs or Amazon S3. After creating flow logs we can retrieve and view its data in the chosen destination.

          Flow logs can help you with number of tasks such as:

          i) diagnosing overly restrictive security group rules.

         ii) Monitoring traffic that is reaching to instance

        iii) determining the direction of traffic to and from the network interface.

         Accept ok record is for originating ping that was allowed by both network ACL and the security group

         Reject record is for originating ping that was denied by both network ACL and the security group

 



16.what is default traffic allowed in NACL ?

Ans: NACL allows all inbound and outbound IPV4 traffic



17.why do we have route table and where do we attach it ?

Ans: With the help of route table we keep track of path and uses these to determine which way to forward traffic.

          it connects different availability zone together and connect VPC to the internet gateway.



19.Do we have any alternate option for vpc peering ? Have you heard about Transient gateway ?

Ans: Yes we have alternate option for vpc peering i.e Transit gateway.

          Transit gateway is network transit hub that can use to interconnect your virtual private clouds and on premises networks.

           As cloud infrastructure expands globally, inter region peering connects transit gateway together using AWS global infrastructure.

===================================================================================================================================================================

\*\*\*Cloudfront:



1.what is the use of Content Delivery Network or Cloudfront ? where you will use it ?

Ans: A CDN (content delivery network), also called a content distribution network, is a group of geographically distributed and interconnected servers.

          Amazon CloudFront is a web service that speeds up distribution of your static and dynamic web content, such as .html, .css, .js, and image files, to your users.

          Amazon cloudfront is used for low latency.



2.tell me where u have used this service in your current organization ?

Ans: I have used CloudFront service in my current organization where we needed low latency means where fast response was expected. for example when we have to do some live streaming then along with network load balancer we have used CDN



3.what is edge location in CloudFront ?

Ans: Edge Location is an AWS data center designed to do fast delivery of services to any users at any location. An edge location is where end users access services located at AWS, the cloud computing division of US-headquartered Amazon. They are located in most of the major cities around the world and are specifically used by CloudFront (CDN) to distribute content to end users to reduce latency.



4.what is CloudFront regional edge cache ?

Ans: Regional edge caches are CloudFront locations that are deployed globally, close to your viewers. They're located between your origin server and the POPs—global edge locations that serve content directly to viewers.

===================================================================================================================================================================

\*\*\*CLOUDWATCH

\*\*\*1.  which service do you use in your organization for monitoring purpose ?

OR

Tell me what different  monitoring services you have used in your organization

Ans: we use CloudWatch for monitoring aws resources



2\. For what purpose and HOW do you use AWS CloudWatch service in your current organization ?

Ans: In my current organization i use AWS CloudWatch to monitor AWS resources and applications we run on aws in real time. i have used CloudWatch to collect and track metrics which are variables for our resources and applications.



3\. tell me what diff metrics you use to monitor in your organization using CloudWatch ?

Ans: i) Cpu utilization

         ii) Disk read operation

        iii) Disk write operation

        iv) Disk read bytes

        v) Disk write bytes

        vi) MetadataNoToken

      viii) Network In

       ix) Network out

       x) Network Packets In

       xi) Network packets out



4\. Which metrices by default are NOT monitored by Cloudwatch ?

Ans:  RAM and disk size cannot be monitor by CloudWatch directly. for that we have used AWS SSM services then we monitored it.

 

4\. which CloudWatch dashboards you have created in your organization and why ?

Ans: for CPU , RAM , DISK



5\. Can we monitor RAM and Disk space  using Cloudwatch ?if not then how ?

Ans: By default we can not monitor RAM and Disk space using cloudwatch. I will use SSM service. I will install cloudwatch agent packge into that server whose RAM and Disk space want to monitor.

          I will create IAM role and will give necessary permissions.



\*\*6. What other activites or automation can be done once we receive the cloudwatch alarm ?

Ans: we can terminate the instance, we can do auto scaling



7\. how can we monitor the application logs in aws.

ans: we can monitor application logs using cloudwatch logs, first we need to install cloudwatch agent on all the required server and configure the path /var/log/app.log



8\. how can we monitor a network logs in aws.

ans : for that we need to use VPC flow logs, it Captures network traffic metadata for VPC, and from VPC flow log we can directly send to s3 bucket.



===================================================================================================================================================================

\*\*\* SNS



1\. what is SNS used for ?

Ans: AWS SNS is a web service that automates the process of sending notifications to the subscribers attached to it. In our project we used SNS to get notification of alarms which we have created on our office mail ids

 

2\. what is TOPIC in SNS ?

Ans: An Amazon SNS topic is a logical access point that acts as a communication channel. A topic allows to group multiple endpoints (such as AWS Lambda, Amazon SQS, HTTP/S, or an email address).



Topic is a communication channel that allows you to broadcast messages to multiple subscribers at once.


   To broadcast the messages of a message-producer system (for example, an e-commerce website) working with multiple other services that require its messages (for example, checkout and fulfillment systems), we can create a topic for your producer system.



3. What are the different options available in SNS which we can use for getting the notifications ?

Ans: Amazon SNS enables us to send notifications directly to customers.

         Amazon SNS supports SMS text messaging to over 200 countries, mobile push notifications to Amazon, Android, Apple, Baidu, and Microsoft devices, and also email notifications.

         Amazon SNS provides redundancy across multiple SMS providers, and enables us to send mobile push notifications using a single API for all mobile platforms.



4\. Have you heard about FIFO in SNS ?

Ans: FIFO is first in first out,  FIFO topics manage ordering and deduplication similar to FIFO queues. We can use IFO topics and queues together to simplify the implementation of applications where the order of operations and events is critical, or when you cannot tolerate duplicates.

          For example, to process financial operations and inventory updates, or to asynchronously apply commands that you receive from a client device.

          FIFO queues can use message filtering in FIFO topics to selectively receive only a subset of messages rather than every message published to the topic.

===================================================================================================================================================================

\*\*\* S3

1.What is the difference between S3 , EFS and EBS ?

Ans: Only EBS volume can be used as root volume to attach for EC2.

     only EFS can be attached to multiple servers at the same time.



2.Can we attach one EBS volume to multiple servers at the same time ?

Ans: No, we can attach one EBS volume to multiple servers at the same time.



3.Can we attach EFS to multiple servers at the same time ?

Ans: Yes, we can attach EFS to multiple servers at the same time.



5.What is cross region replication ?

Ans: With cross-region replication, every object uploaded to an S3 bucket is automatically replicated to a destination bucket in a different AWS region that you choose.

          For example, you can use cross-region replication to provide lower-latency data access in different geographic regions.



6.Can we do cross account replication ? how ?

Ans: To configure replication when the source and destination buckets are owned by different AWS accounts

           i) In this example, you create source and destination buckets in two different AWS accounts. You need to have two credential profiles set for the AWS CLI (in this example, we use acctA and acctB for profile names).

         ii) Follow the step-by-step instructions in Configuring for buckets in the same account with the following changes:

            - For all AWS CLI commands related to source bucket activities (for creating the source bucket, enabling versioning, and creating the IAM role), use the acctA profile. Use the acctB profile to create the destination bucket.

            - Make sure that the permissions policy specifies the source and destination buckets that you created for this example.

 

7.What is mean by versioning and what is the use of versioning ?

Ans: Versioning in Amazon S3 is a means of keeping multiple variants of an object in the same bucket.

     S3 Versioning feature can be used to preserve, retrieve, and restore every version of every object stored in your buckets.





\*\*8.What is life cycle policy in S3 ?

OR

Suppose I want to move data from one storage class to other class then how you will do it ?

Ans: this is used for cost saving purpose. if i want to move data from one storage to other then we use life cycle policy. in my company i used to move data from standard storage class to glacier storage class.





9.How to host static website ?



Go to S3 Console → Create bucket. Enter a unique bucket name (e.g., my-static-site).Choose a region. Uncheck “Block all public access.”, Open the bucket → Upload your HTML, CSS, JS files.

Example: index.html, error.html. Go to Permissions → Bucket policy and add this (replace my-static-site with your bucket name), Go to Properties → Static website hosting. Choose “Use this bucket to host a website.” Specify: Index document: index.html Error document: error.html



Save changes.



Confirm acknowledgment and create the bucket.

10.What is CORS in S3 ?

Ans: Cross-origin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain.

           With CORS support, we can build rich client-side web applications with Amazon S3 and selectively allow cross-origin access to your Amazon S3 resources.



11\. Suppose there is one S3 bucket and there are 5 files in that bucket. I want to give access to one use only to the one file. how can we give it ?

You can give access to only one file in an S3 bucket using an S3 bucket policy or object ACL, but the cleanest way is with a bucket policy that targets just that file.



12\. what is data acceleration in S3 ?

Ans: Amazon S3 Transfer Acceleration is a bucket-level feature that enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket.

         Transfer Acceleration is designed to optimize transfer speeds from across the world into S3 buckets.



13\. what are different storage classes in S3 ? explain them ?

Ans: i) S3 Standard

         ii)  S3 Intelligent-Tiering

        iii)  S3 Standard-Infrequent Access (S3 Standard-IA)

        iv) S3 One Zone-Infrequent Access (S3 One Zone-IA)

        v)  S3 Glacier Instant Retrieval

       vi)   S3 Glacier Flexible Retrieval (formerly S3 Glacier)

       vii)  Amazon S3 Glacier Deep Archive (S3 Glacier Deep Archive)

       viii) S3 Outposts



        The S3 storage classes include "S3 Intelligent-Tiering" for automatic cost savings for data with unknown or changing access patterns.

        "S3 Standard" for frequently accessed data.

        "S3 Standard-Infrequent Access (S3 Standard-IA)" and "S3 One Zone-Infrequent Access (S3 One Zone-IA)" for less frequently accessed data.

        "S3 Glacier Instant Retrieval" for archive data that needs immediate access.

        "S3 Glacier Flexible Retrieval (formerly S3 Glacier)" for rarely accessed long-term data that does not require immediate access.

         and Amazon S3 Glacier Deep Archive (S3 Glacier Deep Archive) for long-term archive and digital preservation with retrieval in hours at the lowest cost storage in the cloud.

         If you have data residency requirements that can’t be met by an existing AWS Region, you can use the S3 Outposts storage class to store your S3 data on premises.

         Amazon S3 also offers capabilities to manage your data throughout its lifecycle.

         Once an S3 Lifecycle policy is set, your data will automatically transfer to a different storage class without any changes to your application.



14.suppose i want to give access to one particular bucket only or one particular file only , how you will do it ?

\- I will use the concept of Access control list ( ACL )  or attach the policy to that bucket or file

\- Or attached appropriate policies to the IAM user.



15\. How you will integrate connection between S3 and EC2 ?

\- I will attach one IAM role to ec2 machine and will give the s3 permissions to that role.



===================================================================================================================================================================

\*\*\*IAM :



1.How do you provide security to your user ?

OR

What is MFA ?

Ans: Multifactor authentication (MFA) is a security technology that requires multiple methods of authentication from independent categories of credentials to verify a user's identity for a login or other transaction.



2.Difference between IAM role and IAM Policies ?

Ans: IAM roles define the set of permissions for making AWS service request whereas IAM policies define the permissions that you will require.

 

3.What is the use of Groups ?

Ans: User groups let you specify permissions for multiple users, which can make it easier to manage the permissions for those users.

          For example, you could have a user group called Admins and give that user group typical administrator permissions. Any user in that user group automatically has Admins group permissions.



\*\*4.What is assume role policy ? explain it with example ?

or

Suppose there are two AWS account - A \& b. Now i want to sit in account A and want to execute some commands in account B. can we do it using assume role policy ?

Ans:





\*\*5.How to integrate EC2 with S3 ?

Ans: need to attach iam role ( where we will give required permissions to that role ) to EC2 machine.



6\. why we use permission boundary ?

\- to provide additonal security to the account. IAM user cannot do any activity even though they have the permissions.

 



===================================================================================================================================================================

\*\*\*TRUSTED ADVISOR

1\. Why you have used trusted advisor in your company ?

Ans: AWS Trusted Advisor provides recommendations that help you follow AWS best practices. Trusted Advisor evaluates your account by using checks. These checks identify ways to optimize your AWS infrastructure, improve security and performance, reduce costs, and monitor service quotas.



2\. what are the different parameters u can monitor using trusted advisor ?

Ans: i) Cost optimization

        ii)  Performance

        iii)  Security

        iv)  Fault tolerance

         v)  Service quotas



3.What is the use of AWS Trusted advisor services ? explain in detail ?

Ans:  AWS Trusted Advisor helps to focus on the most important recommendations to optimize cloud deployments, improve resilience, and address security gaps.

          Trusted Advisor Priority provides prioritized and context-driven recommendations that come from your AWS account team as well as machine-generated checks from AWS Services.

          Trusted advisor helps to monitor 5 parameters

            i) Cost optimization : Trusted Advisor helps to save cost by analyzing usage, configuration and spend.

                                                     Examples include identifying idle RDS DB instances, underutilized EBS volumes, unassociated Elastic IP addresses, and excessive timeouts in Lambda functions.

          ii)  Performance: Trusted Advisor helps to improve the performance of your services by analyzing usage and configuration.

                                           Examples include analyzing EBS throughput and latency, compute usage of EC2 instances, and configurations on CloudFront.

        iii)  Security: Trusted Advisor helps to improve the security of your AWS environment by suggesting foundational security best practices curated by security experts.

                                 Examples include identifying RDS security group access risk, exposed access keys, and unnecessary S3 bucket permissions.

        iv)  Fault tolerance: Trusted Advisor helps to improve the reliability of your services.

                                               Examples include examining Auto scaling EC2 groups, deleted health checks on Route 53, disabled Availability Zones, and disabled RDS backups.

         v)  Service quotas: Service quotas are the maximum number of resources that you can create in an AWS account.

                                              AWS implements quotas to provide highly available and reliable service to all customers, and protects you from unintentional spend.

                                             Trusted Advisor will notify you once you reach more than 80% of a service quota. You can then follow recommendations to delete resources or request a quota increase.

 

===================================================================================================================================================================

\*\*\*Cloud trail

1.What is the use of cloudtrail service ? explain with example ?\\

Ans: we can use CloudTrail to view, search, download, archive, analyze, and respond to account activity across your AWS infrastructure.

          we can identify who or what took which action, what resources were acted upon, when the event occurred, and other details to help you analyze and respond to activity in your AWS account.



2\. Difference between Cloud watch and Cloudtrail ?

Ans: Cloudwatch is used to monitor the resources

     cloudtrail is used to monitor the IAM users activity. It helps us for the auditing logs purpose.



3\. where do we store the cloudtrail logs ?

Ans: We stored it in S3S

===================================================================================================================================================================

\*\*\*ROUTE53



1\. Have you used DNS service ?

Ans: DNS servers translate requests for names into IP addresses, controlling which server an end user will reach when they type a domain name into their web browser.



2\. why it is called as rout53 ? why 53 is used ?

Ans: AWS Route 53 takes its name with reference to Port 53, which handles DNS for both the TCP and UDP traffic requests.



3\. explain me the complete process how  you have integrated the URL from route 53 to load balancer and then to Servers ?

Ans:

         i) Sign in to the AWS Management Console and open the Route 53 console at https://console.aws.amazon.com/route53/

         ii) In the navigation pane, choose Hosted zones.

        iii) Choose the name of the hosted zone that has the domain name that you want to use to route traffic to your load balancer.

         iv) Choose Create record.

          v) Specify the following values:

             Routing policy-

                 Choose the applicable routing policy.

             Record name -

                  Enter the domain or subdomain name that you want to use to route traffic to your ELB load balancer. The default value is the name of the hosted zone.

                  For example, if the name of the hosted zone is example.com and you want to use acme.example.com to route traffic to your load balancer, enter acme.

            Alias

                  If you are using the Quick create record creation method, turn on Alias.

            Value/Route traffic to

                  Choose Alias to Application and Classic Load Balancer or Alias to Network Load Balancer, then choose the Region that the endpoint is from.

                  If you created the hosted zone and the ELB load balancer using the same AWS account – Choose the name that you assigned to the load balancer when you created it.

                  If you created the hosted zone and the ELB load balancer using different accounts – Enter the value that you got in step 1 of this procedure.

            Record type

                  Choose A – IPv4 address.

 

       vi) Choose Create records.

                   Changes generally propagate to all Route 53 servers within 60 seconds.

                   When propagation is done, you'll be able to route traffic to your load balancer by using the name of the alias record that you created in this procedure.





4. what are different record sets ? tell me which record set will be used when ?

Ans:

             A record type:  A record is used to route traffic to a resource, such as a web server, using an IPv4 address in dotted decimal notation

             AAAA record type: AAAA record is used to route traffic to a resource, such as a web server, using an IPv6 address in colon-separated hexadecimal format.

             CAA record type: A CAA record specifies which certificate authorities (CAs) are allowed to issue certificates for a domain or subdomain. Creating a CAA record helps to prevent the wrong CAs from issuing certificates for your domains.

             CNAME record type: A CNAME record maps DNS queries for the name of the current record, such as acme.example.com, to another domain (example.com or example.net) or subdomain (acme.example.com or zenith.example.org).

             DS record type: A delegation signer (DS) record refers a zone key for a delegated subdomain zone.

            MX record type: An MX record specifies the names of your mail servers and, if you have two or more mail servers, the priority order. Each value for an MX record contains two values, priority and domain name.

           NAPTR record type: A Name Authority Pointer (NAPTR) is a type of record that is used by Dynamic Delegation Discovery System (DDDS) applications to convert one value to another or to replace one value with another. For example, one common use is to convert phone numbers into SIP URIs.

           NS record type: An NS record identifies the name servers for the hosted zone.

           PTR record type: A PTR record maps an IP address to the corresponding domain name.

          SOA record type: A start of authority (SOA) record provides information about a domain and the corresponding Amazon Route 53 hosted zone.

 

5\. what are the different routing policies ? explain me with example ?

Ans: i) Simple routing policy: Use for a single resource that performs a given function for your domain, for example, a web server that serves content for the example.com website. You can use simple routing to create records in a private hosted zone.

         ii) Failover routing policy: Use when you want to configure active-passive failover. You can use failover routing to create records in a private hosted zone.

         iii) Geolocation routing policy: Use when you want to route traffic based on the location of your users. You can use geolocation routing to create records in a private hosted zone.

        iv) Geoproximity routing policy: Use when you want to route traffic based on the location of your resources and, optionally, shift traffic from resources in one location to resources in another.

        v) Latency routing policy: Use when you have resources in multiple AWS Regions and you want to route traffic to the region that provides the best latency. You can use latency routing to create records in a private hosted zone.

        vi) IP-based routing policy: Use when you want to route traffic based on the location of your users, and have the IP addresses that the traffic originates from.

        vii) Multivalue answer routing policy: Use when you want Route 53 to respond to DNS queries with up to eight healthy records selected at random. You can use multivalue answer routing to create records in a private hosted zone.

       viii) Weighted routing policy: Use to route traffic to multiple resources in proportions that you specify. You can use weighted routing to create records in a private hosted zone.

 

6\. suppose there are 2 applications  which are running in 2 different regions. now you want to make sure if one region goes down then traffic should flow to region-2 then how you will make sure ? which routing policy you will use ?

Ans: I will use fail over or weighted based routing policy.







===================================================================================================================================================================

\*\*\*Guard Duty,Config,SSM (System Manager),Lambda



1\. what is the use of guard duty ?

Ans: Amazon GuardDuty is a threat detection service that continuously monitors your AWS accounts and workloads for malicious activity and delivers detailed security findings for visibility and remediation.



2\. what is the use of config service ?

Ans: config service is an externalized application configuration service.

          It is based on the open-source Spring Cloud Config project, which provides a centralized server for delivering external configuration properties to an application and a central source for managing this configuration across deployment environments.



\*3. tell me where have you used AWS SSM service in your organization ?

Ans:

\- i used this service : for monitoring RAM and disc space alerts

\- to execute the linux commands on multiple ec2 machines at the same time

4. we what to migrate our database from gp2 to gp3 in all the servers in aws, how can we do that. 

we can use ssm with is a system manager service in aws, first we need ssm agent to be installed on all the servers, and then using the run command we can run below command on all the servers and it will update automatically 

aws ec2 modify-volume --volume-id vol-0123456789abcdef0 --volume-type gp3
 
5. how we can check how many servers are associate with the ssm service to do any changes. 

Systems Manager → Fleet Manager → Managed instances 

6. we want our ubuntu servers to be updated every week sunday night so how will yo do that. 

I will use SSM service, there is run command section where we will get the access of aws-runpatchbaseline document, will select all server and apply, we can automate this also using mainteninance window  




====================================================================================================================================================================

\*\*\*GENERAL :



1\. How you will monitor :-

    a. Server metrics  ( using cloudwatch )

    b. System logs ( in /var/log/messages folder)

    c. Networking logs (vpc flowlogs)

    d. Application logs ( CloudWatch log groups)



2\. How you will build the 3 tier architecture or 2 tier architecture ?   ( recording is provided )

To build 3 tier architecture, firstly i will make sure my network setup is ready which includes vpc,subnets and routes configuration.

Then i will have my front end, backend and storage deployed. Front end would be my load balancer ,which will run in public facing .backend would be ec2 in pvt subt and storage can be either database in pvt subnet or s3.Just to make sure there is  high availability and fault tolerance i will make sure ec2 are launched in multiple pvt subnets. I will autoscaling for the same.



Once my setup is ready then i will use some  services to monitor and keep env secure like cloudwatch,cloudtrail,guard duty,CDN,WAF,Trusted advisor etc



3\. explain me what are the different services you have used in your organization ?

\- EC2,load balancer , autoscaling, VPC,VPC Flowlogs,IAM,CDN,Trusted advisor,SSM,Cloudwatch,Cloudtrail, SNS,Config etc



4\. tell me how you will secure ( security ) to your AWS project / AWS infrastructure ?

\- Using trusted advisor, NACL,Security group,MFA,WAF,Gurad duty



5\. what is mean by high availability ( HA )? how you will take care of it while building your project ?

\- HA means even if server running in subnet goes down you should have other server running in different subnet. so for HA its better to launch ec2 machines in different subnets rather than launching all of them in one subnet.



6\. how you will differentiate 2 environments like production and testing ?

\- I will have separate vpc for production environment and separate vpc for testing environment.





error code meaning



400 – Bad Request: The request is malformed or invalid (e.g., syntax error, missing parameter).



401 – Unauthorized: Authentication required or failed (invalid token, missing credentials).



402 – Payment Required: Reserved for future use, sometimes used for paid APIs.



403 – Forbidden: Server understood the request but refuses to authorize it (user lacks permission).



Server-side errors (5xx):



500 – Internal Server Error: Generic server failure; unexpected condition occurred.



501 – Not Implemented: The server doesn’t support the functionality required to fulfill the request.



502 – Bad Gateway: Server received an invalid response from an upstream server (proxy or gateway issue).



503 – Service Unavailable: Server temporarily overloaded or down for maintenance.





\####################################################################################################################################################################################

\############################################# GIT and GIT HUB #######################################################################################################################

\#####################################################################################################################################################################################







1\. What is Git?



Git is a Distributed Version Control system(DVCS). It lets you track changes made to a file and allows you to revert back to any particular change that you wish.



2\. What is the difference between Git and Github?

Git is a version control system, while GitHub is a cloud-based hosting platform for Git repositories.



3\. What is a commit message?

The command that is used to write a commit message is “git commit -a”. Now explain about -a flag by saying -a on the command line instructs git to commit

the new content of all tracked files that have been modified.



4\. How can you fix a broken commit?

In order to fix any broken commit, use the command “git commit --amend”. When

you run this command, you can fix the broken commit message in the editor.



5\. What is a repository in Git?

Repository in Git is a place where Git stores all the files. Git can store the files either on the local repository or on the remote repository.



6\. How can you create a repository in Git?

To create a repository, create a directory for the project if it does not exist, then run the command “git init”. By running this command .git directory will be created in the project directory.



7\. What is ‘bare repository’ in Git?

A bare repository in Git is a repository without a working directory.



8\. What is a ‘conflict’ in git?

Git can handle on its own most merges by using its automatic merging features. There arises a conflict when two separate branches have made edits to the same line in a file, or when a file has been deleted in one branch but edited in the other. Conflicts are most likely to happen when working in a team environment.



9\.  How is git instaweb used?

git instaweb = quick, local, browser-based view of your Git repo’s history and contents.



10\. Below are some basic Git commands



git rm \[file]  - deletes the file from your working directory and stages the deletion.

git log  - list the version history for the current branch.

git show \[commit] - shows the metadata and content changes of the specified commit.

git tag \[commitID] - used to give tags to the specified commit.

git checkout \[branch name] - used to switch from one branch to another.

git checkout -b \[branch name] - creates a new branch and also switches to it.



11. In Git how do you revert a commit that has already been pushed and made public?

git revert <name of bad commit>



12\. What is the difference between git pull and git fetch ?

Git pull command pulls new changes or commits from a particular branch from your central repository and updates your target branch in your local repository.



When you perform a git fetch, it pulls all new commits from the desired branch and stores it in a new branch in your local repository. If you want to reflect these changes in your target branch, git fetch must be followed with a git merge.



13\. What is ‘staging area’ or ‘index’ in Git?



That before completing the commits, it can be formatted and reviewed in an intermediate area known as ‘Staging Area’ or ‘Index’. From the diagram it is evident that every change is first verified in the staging area I have termed it as “stage file” and then that change is committed to the repository.



14\.  What work is restored when the deleted branch is recovered?

The files which were stashed and saved in the stash index list will be recovered back. Any untracked files will be lost. Also, it is a good idea to always stage and commit your work or stash them.



15\.  What is git stash?

git stash temporarily saves your uncommitted changes (both staged and unstaged) so you can work on something else without committing them.



16\. What is the difference between the ‘git diff ’and ‘git status’?





git status Shows which files have changes (staged, unstaged, or untracked).

git diff Shows what changes were made inside the files (line-by-line differences).



17\. What is the difference between ‘git remote’ and ‘git clone’?



‘git remote add’ creates an entry in your git config that specifies a name for a particular URL

‘git clone’ creates a new git repository by copying an existing one located at the URL



18\. What is the function of ‘git config’?

Git uses your username to associate commits with an identity. The git config command can be used to change your Git configuration, including your username.



19\. Describe the branching strategies you have used.



we use git flow strategies



feather branch -> development branch -> QA branch -> master branch and there is one more branch called Hotfix branch



20\. How will you know in Git if a branch has already been merged into master?



go to master branch and hit - git branch --merged

will show all the branches which are merged



21\.  In Git, how would you return a commit that has just been pushed and made open?



git revert HEAD~2..HEAD



22\.  How to remove a file from git without removing it from your file system?



git reset <filename>



23\.  Tell me the difference between HEAD, working tree and index, in Git.



The working tree/working directory/workspace is the directory tree of (source)

files that you are able to see and edit.

 

 The index/staging area is a single, large, binary file in

<baseOfRepo>/.git/index, which lists all files in the current branch, their SHA

1 checksums, timestamps, and the file name – it is not another directory

which contains a copy of files in it.



 HEAD is used to refer to the last commit in the currently checked-out branch.



24\. difference between git revert and git reset



suppose you have moved wrong changes from working area to staging area and you did git reset, it will delete only last commit that you have done.

and suppose you have committed the changes in that case it git revert will create another commit which will undo the changes from previous commit



25\. What is git reflog?

git reflog shows a log of all reference updates in your local repository — including commits, checkouts, merges, resets, and rebases.





\#######################################################################################################################################################################################

\############################################# Docker ###########################################################################################################################################################################



1.what are the advantages of using docker ? why we use docker ? which containerization tool you have used in your organization ?

\- cost saving since we will create container only with the amount of ram and cup required to run that microservice.

\- Rapid deployment

\- same docker image can be used on multi cloud platform

\- more secure.





2\. how to reduce the docker image size ?

\- use multistage concept

\- smaller base images (lightweight images like alpine)

\- reduce the number of layers.( instead of adding multiple run instruction use only one time RUN instruction and write evrything infront of it)

\- ignore unwanted files using dockerignore





2.what is dockerfile ?

  -- docker file is a simple textfile that carries the detail information using which docker image will be created. we write dockerfile using different instructions.

     Dockerfile includes information like operating system details , executing any commands , downloading packages , dependencies etc.



3.what is docker image ?

  -- They are executable packages(bundled with application code \& dependencies, software packages, etc.) for the purpose of creating containers.

 Docker images can be deployed to any docker environment and the containers can be spin up there to run the application.



4.what are the different instructions you have used to write docker file ?

A.FROM: This is used to set the base image for upcoming instructions. "A docker file is considered to be valid if it starts with the FROM instruction"

B.LABEL: This is used for the image organization based on projects, modules, or licensing.

C.RUN: When we want to execute the commands

D.CMD: This command is used to provide default values of an executing container. In cases of multiple CMD commands the last instruction would be considered.

ENTRYPOINT : this also gets executed when container gets created.

EXPOSE

MAINTAINER

USER

WORKDIR

VOLUME



5.diff between ADD \& COPY ?

-- COPY provides just the basic support of copying local files into the container

-- ADD provides additional features like remote URL  ( downloading packages from internet ) and tar extraction support.



6.diff between CMD \& ENTRYPOINT

   CMD : gets executed when container will be created.it can be override.

   ENTRYPOINT : gets executed when container is created. priority of ENTRYPOINT Is high. cannot be override.



7.Suppose i want to do all activities using some specific user , which instruction you will use ?

USER



8.what is port forwarding / port mapping in docker ?

docker run -d -p 8080:80 nginx



9.how you will login into existing docker container ?

docker exec -it <container\_name\_or\_id> /bin/sh



10.how to take backup image of any running container ?

docker commit <container\_name\_or\_id> <new\_image\_name>:<tag>





11.command to check all running container ?

docker ps -l



12.command to check all container ( running and stop both) ?

docker ps -la



13.can we delete running container ?

  - we can delete it forcefully but its not good practise. we must stop the container and we should delete it.



14.where do you store your docker images ?

we store on ecr.



15.if ecr then how you push your images ?

 docker tag <local\_image\_name>:<tag> <ECR repo URL>





17.what is docker lifecycle ?

--- The different stages of the docker container from the start of creating it to its end are called the docker container life cycle.

The most important stages are:



A.Created: This is the state where the container has just been created new but not started yet.

B.Running: In this state, the container would be running with all its associated processes.

C.Paused: This state happens when the running container has been paused.

D.Stopped: This state happens when the running container has been stopped.

E.Deleted: In this, the container is in a dead state.



18.where do you store  the data of your containers ?

   - we use persistent volume.

   - They are stored in Docker host filesystem at /var/lib/docker/volumes/



19.command to create docker volume ?

 docker volume create <volume\_name>





20.Diff types of network drivers ? explain them ?

     - Bridge network  - this is by default network which gets used.

     - local  - this is generally used when we want to assign same IP address to container as that of local machine.

     - null   - It means there will be no IP address to contianer.No internet access. cannot be access from outside the server. this is used only for testing purpose.



21. Which orchestration tool you know ?

  There are different orchestration tools like AWS rancher kubernetes. I have used Kubernetes. for deploying cluster on AWS, i used AWS EKS service.



22. Have you used AWS ECS service ?

  No. i have used AWS EKS service for deploying the microservices on Kubernetes cluster.



23\. what is docker daemon ?

 The Docker daemon ( dockerd ) listens for Docker API requests and manages Docker objects such as images, containers, networks, and volumes.



24\. what is docker-compose ? have you used it in your organization ?

It is a YAML file consisting of all the details regarding various services, networks, and volumes that are needed for setting up the Docker-based application. So, docker-compose is used for creating multiple containers, host them and establish communication between them. For the purpose of communication amongst the containers, ports are exposed by each and every container.



I have used this mostly when we are doing testing. we need to create again and again same containers so our developers , testers can do their testing. so i use docker-compose to create those container. this saves my time.



25\. What is docker image registry? which tools do you use in your organization to store docker images ?

A Docker image registry, in simple terms, is an area where the docker images are stored. Instead of converting the applications to containers each and every time, a developer can directly use the images stored in the registry.

This image registry can either be public or private and Docker hub is the most popular and famous public registry available.

some other tools are - AWS ECR , Nexus



26\. Why you store images on dockerhub and not on AWS ECR ?

\- AWS ECR is private repository which can be used only for AWS. in our company there are different projects running on different cloud hence we use dockerhub.

\- this makes easy for  everyone to pull the docker images.



27\. what you do for security of dockerimages ?

\- we use the concept of trivy scanner. this scans our docker images. if we find some vulnerabilities then we clear them and then only we push our docker image to docker hub



28\.  How many Docker components are there?

-- There are three docker components, they are - Docker Client, Docker Host, and Docker Registry.



A.Docker Client: This component performs “build” and “run” operations for the purpose of opening communication with the docker host.

B.Docker Host: This component has the main docker daemon and hosts containers and their associated images. The daemon establishes a connection with the docker registry.

C. Docker Registry: This component stores the docker images. There can be a public registry or a private one. The most famous public registries are Docker Hub and Docker Cloud.



29.Can you tell the differences between a docker Image and Layer?

-- Image: This is built up from a series of read-only layers of instructions. An image corresponds to the docker container and is used for speedy operation due to the caching mechanism of each step.



-- Layer: Each layer corresponds to an instruction of the image’s Dockerfile. In simple words, the layer is also an image but it is the image of the instructions run.



30\. How will you ensure that a container 1 runs before container 2 while using docker compose?



\- The “depends\_on” parameter helps us to do it.


31. how to check logs of docker container 

docker logs <container_id or container_name> 

32. how to check the IP of docker container 

docker inspect <container_id_or_name>

33. what is the command to delete all the docker images in just one command 

docker rmi -f $(docker images -q)


34. how can we check what commands docker executed to create the docker image 

docker history <image> using this we can check the image layers \


]version: "2.4"

services:

 backend:

   build: .

   depends\_on:

     - db

 db:

   image: postgres



31\. command to create docker image , docker container

docker build -t my\_app:latest .

docker run -d --name container image:tag



\###################################################################################################################################################################################

\############################################################# terraform ##########################################################################################################

\#####################################################################################################################################################################################



1.Where you have used terraform in your current company  what kind of work you have done using 

terraform  

Solution -  I have written terraform template which help to built the completed environment. my 

template included vpc  subnet  internet gateway  aws s3  security group  aws ec2.. 



2.What is terraform taint  

It updates the Terraform state file to indicate that a resource is “tainted.” When you run terraform apply, Terraform will destroy and re-create that resource. terraform taint forces Terraform to recreate a resource that exists but isn’t functioning as expected.



OR

&nbsp;

2.Suppose I want to change only one particular resource and don’t want to make any changes in existing 

infrastructure then how you will do it   

terraform apply -target=<resource\_address>



3.What is tfstate file

It tracks which real-world resources correspond to your Terraform configuration.

Example: When you run terraform apply, Terraform updates this file to record the current infrastructure status.

&nbsp; 

4.Where do you store the tfstate file  how syntax example 

we store our tfstate file in s3 bucket because the file should be accessible to all the team members. 

&nbsp;

5.Have you use provisioner concept in your day to day life while writing the terraform template  give 

example  

yes, there are 3 types of provisioner in terraform, local exec remote exec and file provisioner, file provisioner we use when we want to apply the script from our system which is present in same file structure, remote exedc we use when we want to apply few commands in the servers which we are going to create, and local exec we use when we want to apply command in our local system after the execution. 



6.What you did to remove hard coded values in your template  what are the advantage of using 

variable concept  give example. 

to remove the hard coding we use terraform variables, using which we can declare the global variables values and we can import those values in our main file, using this we can reuse the same main file with different values. 

&nbsp;  

7.What is the interpolation concept  

combining variables, resources, and functions to build dynamic values in Terraform configuration.



OR 

How you will use  call the newly created vpc id while creating other resources like subnet , ec2 etc  

Use the resource reference to call the VPC ID from the resource that created it. Terraform automatically makes outputs from one resource available to others in the same configuration.



8.what is provider in terraform template 

A provider in Terraform is a plugin that tells Terraform how to interact with a specific cloud or service (like AWS, Azure, GCP, Docker, GitHub, etc.).

&nbsp;

9.what will happen if I remove any resource name from tfstate file and then try to run terraform destroy 

If you remove a resource entry from the tfstate file and then run terraform destroy, Terraform will not destroy that resource in your cloud environment.



commands 

10\. what is the use of terraform init command

terraform init initializes a Terraform working directory before you run any other command. Download providers, Initialize backend, Prepare modules, Verify configuration. 



11\. name diff commands ( atleast 8-9 ) which you have used in your daily work  

terraform init – Initializes the working directory, downloads providers, and sets up backend.



terraform plan – Shows what changes Terraform will make before applying them.



terraform apply – Creates or updates infrastructure as defined in .tf files.



terraform destroy – Destroys the infrastructure managed by Terraform.



terraform fmt – Formats .tf files to follow Terraform’s standard style.



terraform validate – Checks syntax and verifies configuration is valid.



terraform show – Displays the current state or details of a saved plan.



terraform output – Prints output values from the state file.



terraform state list – Lists all resources tracked in the Terraform state.



terraform taint (deprecated but still seen) – Marks a resource for recreation on next apply.



terraform import – Brings an existing real-world resource under Terraform management.



terraform state rm – Removes a resource from the state file without destroying it.

12.have you  created any modules by your own   if yes , how you call them in your template  

module "vpc" {

&nbsp; source = "github.com/shyam-devops/terraform-modules//vpc?ref=main"

&nbsp; cidr\_block = "10.0.0.0/16"

}



13. What is the use of terraform import  

terraform import is used to bring existing real-world resources under Terraform’s management without recreating them.



14. what are the different types of variables  explain with example  

Terraform supports three main types of variables: Input Variables, Local Variables, and Output Variables.



15. what are the different challenges you faced while working on Terraform 

State file management:

Early on, we faced conflicts when multiple team members tried to apply changes simultaneously. We resolved it by using a remote backend (S3 with DynamoDB locking) to manage state centrally and avoid race conditions. 

Resource drift:

Sometimes, resources were modified manually in the cloud console, causing drift between the actual infrastructure and Terraform state. I handled this using terraform plan and terraform refresh to detect and reconcile changes.

Complex dependency management:

Some resources depended on outputs from others. I used explicit depends\_on and structured modules properly to ensure predictable deployment order. 



16\. write down the use of each command in terraform 



terraform init :- initializes a working directory and downloads the necessary provider plugins and

&nbsp;modules and setting up the backend for storing your infrastructure's state.

&nbsp;terraform plan:- It is used to preview changes that Terraform will make to your infrastructure

&nbsp;before applying them

&nbsp;terraform validate:- To verify the correctness of Terraform configuration files. It checks the syntax

&nbsp;of the Terraform files, ensures the correct usage of attributes and values, and validates the

&nbsp;configuration based on the core syntax of Terraform and also by checking all the providers in the

&nbsp;code.

&nbsp;terraform apply:- It is used to create, update, or delete infrastructure resources based on the

&nbsp;changes specified in an Infrastructure-as-Code (IaC) configuration

&nbsp;terraform destroy:-It is used to remove the infrastructure that has been provisioned using

&nbsp;Terraform Infrastructure-as-Code(IaC) configuration

&nbsp;terraform state list:-It is used to list resources within a terraform state.

&nbsp;terraform fmt :- It is used to rewrite Terraform configuration files to a canonical format and style.

&nbsp;terraform import:-It is used to import existing infrastructure into Terraform state.

&nbsp;terraform replace or terraform taint:-It is used to informs Terraform that a particular object has

&nbsp;become degraded or damaged.

&nbsp;terraform destroy --target <resource name>:

terraform refresh:-It is used to update the state file in Terraform to reflect the current state of

&nbsp;infrastructure that Terraform manages



17\.  how to check detail logs in terraform ? 

Terraform has detailed logs that you can enable by setting the TF_LOG environment variable to
any value.There are 5 log levels. TRACE,DEBUG,INFO,WARN,ERROR. 
then define the path with TF_LOG_PATH and store the logs on that path 



18\. what is the validation block in terraform ? 

In Terraform, the validation block is used inside an input variable definition to enforce custom rules on the variable’s value before Terraform applies the configuration. 

suppose you have created ec2 instance module and you don't want anyone to use it with the variable value t2.large , so you can mention validate block in your module so anyone is trying to created ec2 instance with t2.large it will through error message . 

19. what is mean by tfstate file ? 
 tfstate file contains information about the provisioned infrastructure which terraform manage.
 Whenever we change the configuration file, it automatically determines which part of your
 configuration is already created and which needs to be changed with help of state file

20. where do you store the tfstate file in your company ? 
 tfstate is stored by default in a local file named "terraform. tfstate". After running “terraform init”
 command terraform tfstate file stored in local file.To check state file, we can use “terraform state
 list” command

21.  explain terraform lifecycle ? 
terraform init 
terraform validate 
terraform plan 
terraform apply 

22.  how do you store your tfstate file in S3 bucket ? 

go to aws console, then create a new s3 bucket and enable versoning of the bucket, then go to terraform write a new terraform file and mention backend block inside it and then provide the bucket name and ally the file, once done the tf.state file will automatically get stored in the s3 bucket 

23.  explain me the concept of tfstate file locking ? how you will do it ? 
 Terraform will lock your state for all operations that could write state. This prevents others from acquiring the lock and potentially corrupting your state.” This is helpful so that two people cannot perform the same action at the same time and can prevent issues where two resources of the same type may be deployed and then the state would be messed up. 

 24. explain dynamic block configuration , where you will use it ? 
  Dynamic Blocks in Terraform let you repeat configuration blocks inside a resource, provider,
 provisioned, or data source based on a variable, local, or expression you use inside them

 25. what is use of count and count index ? 
  Count is used to create multiple instances of a resource or module, using a single configuration block. 

  The count. index is a special variable that Terraform creates inside a resource block using count argument. This helps you uniquely identify each instance of the resource. Indices start from 0 and grow by increments of 1 for each resource instance created
 
 26.  what is use of variables in terraform ? 
  In Terraform, variables are used to make your infrastructure code more flexible, reusable, and
 customizable. By defining variables, you allow the values for specific parameters of your
 infrastructure (e.g., instance types, regions, or AMI IDs) to be defined outside the code, making
 it easier to modify the configuration without changing the actual Terraform files

 27.  have you used the concept of modules in your company ? how and why ? 
  Modules in Terraform are a collection of .tf files stored in a separate directory within the overall
 configuration.

 28.  explain the use of mapping and list concept in variables ? 
  List and map variables in Terraform empower users to create dynamic, reusable configurations
 for managing infrastructure. 

 29.  what does the concept of outputs in terraform ? 
  Terraform outputs are structured data about resources that can be exported and used to configure other parts of infrastructure. They are similar to return values in programming languages
 
 30. what are the different type of provisioners in terraform ? explain with example ? 
 Terraform providers are plugins used to authenticate with cloud platforms, services or other tools, allowing users to create, modify, and delete resources declared in the Terraform configurations. Provisioners are used only for copying files or executing local or remote operations.
 There are three types of provisioners in Terraform
 Local-exec provisioners
 Remote-exec provisioners
 File provisioners.

 31. explain me the concept of workspace with example ? 
 A Terraform workspace is a way to create multiple instances of the same infrastructure in different environments. For example, you may have a development environment , production environment. Each environment may require the same infrastructure but with different configurations.

 32. what does .tfvars file means ? 
  A .tfvars file is a Terraform file that stores input variables for configurations. The variables in a
 .tfvars file can be used to parameterize Terraform code and provide dynamic values.

 33.  can we have multiple provides in same template ?
 Yes

 34. how does terraform comes to know that which version of aws provider needs to be used ? 
  Open the terraform.tf file. And then find a terraform block which specifies the required provider version and required Terraform version for this configuration.

 35.  what happens in backend when you execute terrafrom init command ? 
  When you execute the terraform init command in Terraform, it initializes the working directory containing Terraform configuration files and prepares it to interact with the specified backend. This process involves several important steps, including backend initialization, provider
 installation, and module initialization.

 36.  suppose you have launch vpc and subnet using template. now you delete the resource details
 from tfstate file ? what impact will happen on those vpc and subnet ? will they be deleted or not
 ?
 No they will not get deleted 

 37.  suppose you have launch vpc , subnet & ec2 using template , now your manager says that
 please change the instance type of the ec2 which you have created earlier using template. now
 what will happen if you update the template and say terraform apply ? new ec2 will be created or it will update the same server ?

 It will destroy the existing one and create a new ec2
 
 38. what is the purpose of using sensitive parameter in terraform in output ? explain with example ?
 The sensitive parameter in Terraform is used to mark a variable or output as containing sensitive data, such as passwords, API keys, or secrets.
 
 39. what are the errors that you have faced while doing terraform init terraform validate terraform plan and apply.

 terraform init 
 Occurs when the source module path (e.g., GitHub, S3, Terraform Registry) is invalid or network access is blocked. 

 terraform validate 
 Happens when a required variable or argument is not defined in the configuration or variables file.

 terraform plan 
 Occurs when the provider (like AWS) is not initialized properly or missing from the configuration. 

 terraform apply
 Happens due to insufficient IAM permissions for creating or modifying resources in the target cloud environment. 

#############################################################################################################################################
############################################### kubernetes ##################################################################################
##########################################################################################################################################

1. what kind of troubleshooting you do in Kubernetes ?
- we need to provide support to our projects where microservices are running on EKS cluster. so daily we get some issues like :
- pods are in pending state
- image back errors
- loopback errors.

need to deploy some objects , write manifest files etc. 


1. how can we provide access of aws services to a perticuler pod. 
first take a IRSA (IAM user for service accout) ID and go to IAM user, and go to identity provider, and provide your IRSA endpoint and make a thumbprint of it. then create a IAM policy with required access, then create a IAM role, and attach this policy to that role, now you have ARN of that role, create a service accout manifest using that role arn. and use that service accout name in any deployment or pod. 


2. explain me the architecture of Kubernetes cluster ??
Ans - explain master server and worker nodes , and all components like api server , etcd , scheduler , controller , kubelet , kubeproxy , containers.

3.How big is your cluster ?? like how many master nodes and worker nodes are there ??
Ans - 1 master node and 10 worker node.

4.what are the different objects you have created in your organization ??
Ans - I have create objects like pod, deployment , services ,resource limit, persistance volume clame, secrets and config maps, stateful sets, ingress controller.

5. command to check log of pods ??
Ans - kubectl logs podname -n namespace // kubectl logs podname -n namespace -c controller_name

6. what does probes means in kubernetes ??
Ans - Probes are configured to check health of the containers running in a pod. there are 3 types of probes liveness , readyness and startup probes.
liveness prob used to determine whether a particular container is running or not.
readyness prob used to determine whether a particular container is ready to receive requests or not.

7. what does image pull back error means in kubernetes ??
Ans - its means that a container could not start because kubernetes could not pull a container image ( for reasons such as invalid image name , or pulling from a private registry without imagePullSecret)

8.what does crash loop back error means  in Kubernetes ??
Ans - Crash loop backoff is a Kubernetes state representing a restart loop that is happening in a pod. a container in the pod is started , but crashes and is then 
restarted , over and over again.
  one or more containers are failing and restarting repeatedly. this typically happens because each pod inherits a default restartPolicy of always upon creation.

9. which version of kubernetes you have used in your organization ??
Ans - kubernets 1.30

10. what is difference between deployment , daemon sets and stateful sets ?
Ans - "deployment" is used for stateless applications.
    - "stateful" sets is used for stateful applications , each replica of the pod will have its own state and will be using its own volume.
    - "daemonset" is a controller similar to replicaset that ensures that the pod runs on all the nodes of the cluster.
      the pods in a deployment are interchangeable , whereas the pods in a statefulset are not.
       
11. difference between replication controller and relica set ??
Ans -the main difference between this two is the rolling update command works with replication controllers , but won't work with replica set.
     replication contoller uses equity based selectors to manage the pods . replica sets controller uses set-based selectors to manage the pods.

12.Different kubernetes services ?
Ans -Types of services in kubernetes - cluster ip , nodeport load balancer and ingress controller

13. why do we use ingress controller ??
Ans -using ingress controller enables you to easily and securely route traffic to our applications running as kubernetes services.
If we use ingress controller then only single load balancer will be expose to internet. all our microservices will run on cluster ip. 

14. can we do horizontal scaling of pods ??
Ans - yes , horizontal pod auto scaler increase and decrease the pods in response to the workloads cpu or memory utilization.

15. what is your day to day work on kubernetes ?
On a day-to-day basis, I manage and maintain Kubernetes clusters to ensure applications run reliably and efficiently. My key tasks include:

Deployment Management:

Creating and updating Kubernetes manifests (Deployments, Services, ConfigMaps, Secrets, Ingress).
Managing Helm charts for application releases.

Monitoring and Troubleshooting:

Monitoring cluster and pod health using Prometheus and Grafana.
Checking logs with kubectl logs and debugging issues with pods, nodes, or networking.

Scaling and Optimization:
Managing Horizontal Pod Autoscalers (HPA).
Optimizing resource requests/limits and node utilization.

Security and Access Control:
Managing RBAC roles, ServiceAccounts, and Secrets.
Ensuring namespaces are isolated and secured.

Cluster Maintenance:
Upgrading components, applying patches, cleaning unused resources.
Backing up critical data and validating cluster state.

17. write down one manifest file to create pod carrying Apache pod.

pod --- 

apiVersion : v1 
kind : pod 
metadata : 
    name  : my_application 
    lable : 
      app : my_applcation 
spec : 
    containers : 
    - name : my_appcontainers 
      image : httpd:latest 
      port : 
        - containersPort : 80  

18. suppose your pod is in pending state from last 10 mins . what could be the issue. ?
- Pod might not be getting the available node  because of lack of resources like CPU and RAM.

19. what is the use of node selector ? have you used it in your company ?
- supposer we want to deploy some pods on specific node only then we will use node selector.

20. what details are present in kubeconfig file ?
- kubeconfig is a YAML file that contains either a username and password combination or a secure token that when read programmatically removes the need for the Kubernetes client to ask for interactive authentication. kubeconfig is the secure and standard method to enable access to your Kubernetes clusters.

21. How do you provide access to individual users at cluster level ?
We use the concept of RBAC. for this we have written 3 different manifest files like - role , role binding and config files.
roles : it carries the information about the policies /permissions
role binding : details of users to which we want to attach this role
config : aws user details.

22..what will happen if api server is destroyed ? there will be no communication in between master and worker nodes. therefore no further operations can be done
- we will not be able to communicate with the cluster.

23.what will happen if scheduler is not running ? 
-  we wont be able to deploy the containers on the  nodes

24.what will happen if controller is not running ?   
- we wont be able to create the pods automatically

25.what will happen if kubelet is not running in the node ? 
- there will be no communincation between the node ( je chya warcha kubelete chalat nhiye ) and master

26.does container runs directly on all nodes ? no , cont. run inside the pod.
- No . container runs inside the pod.
27.what is pod ? 
-It is nothing but a imaginary boundary within which we launch our containers. suppose your docker containers get failed, then since those containers are launched in pod, they will get created automatically.

28.advantages of Kubernetes ?
  - high availability 
  - Auto scaling
  - auto healing 
  - resource optimization 
  - Load balancer

29 . where do you save the config file ?  we save it in .kube folder  with name config
- it gets created under .kube folder in the users home directory using which we are creating the cluster. 

30 . Suppose i want to decide that on which node my pods should get created then how u will do it ??
- i will use the concept of node selector. : i will give labels to all the nodes and then in my yml file i will use the nodeselector parameter and the label details.

31 . Explain the consept of pod affinity and pod anti affinity 

using the pod affinity and pod anti affinity we can control the scheduling of the pod on to the nodes which we have in our cluster pod affinity means on which node you want to schedule the pod like if there are 2 pods which need low latency in between then you will use pod affinity, and pod antiaffinity means you dont want to schedule all your pod replicas on single node because if that node goes down your application will go down.  

32 . what is service mesh in kubernetes 

A Service Mesh in Kubernetes is a dedicated infrastructure layer that manages service-to-service communication within a cluster.
It provides traffic control, security, and observability without modifying application code. 
service mesh tool - istio 

33. what is afinity in kubernetes 

Node Affinity --> Run pods only on nodes with specific labels (e.g., SSD storage or specific region). 

Pod Affinity -- > Keep microservices that communicate frequently close together to reduce latency. 

Pod anti-affinity --> Spread replicas of the same app across multiple nodes for high availability. 

34. what is the use of HPA in kubernertes 

HPA (Horizontal Pod Autoscaler) automatically scales the number of pod replicas in a Deployment, ReplicaSet, or StatefulSet based on observed metrics. 
If metrics exceed or drop below the target threshold, HPA increases or decreases replicas automatically. 

35. what is the use of kubeproxy 

kube-proxy enables service discovery and load balancing by managing network rules so that traffic reaches the right pods reliably. 

36. my pod is in pending state, what are the possible reason and which step i can follow to troubleshoot this issue ? 

Reason because it is in pending state -> 

Insufficient resources (CPU, memory, storage) on nodes

No matching node affinity / taints / tolerations

Unbound PersistentVolumeClaim (PVC)

----------------------------------------------------

process of troubleshoot 

i will use command - kubectl describe pod <pod-name> | tail -n 20
i will get the events which happend in the pod file like, scheduled, pulling, pulled, created, started.  
 
 mostly we will understand the issue from this, because at the end clearly states why it’s pending.

then will check nodes of my kubernetes with

kubectl get nodes 
kubectl describe nodes 

will check the running processes with the command 

Kubectl top nodes 
Kubectl top pods 

then i will check PVC binding 
kubect get pvc --> PVC should show Bound. If Pending, verify StorageClass or PV availability. 

lastly i will check affinity or nodeSelector

Review pod YAML for nodeSelector, affinity, or topologySpreadConstraints that might prevent scheduling.

also there can be reason like the secrates that you have used in the pod manifest file is wrong there are mo secrates with that name so this might be the reason. 

37. what is the difference between resource quota and (Limit range) service limit ?

Applies at namespace level - resource quota
applies at pod lavel - (Limit range) service limit 

38. can we create deployment with the persistant volume. 
es. You can use Persistent Volumes (PV) and Persistent Volume Claims (PVC) with a Deployment manifest. 

39. what is the diffeence between node selector and node affinity 
nodeSelector is the basic form of node filtering. 
Schedule pods on specific nodes using key-value labels
Simple key-value match
on the other hand. 
nodeAffinity is the advanced and recommended approach for flexible scheduling control.
More advanced, flexible way to control pod placement
Supports expressions and multiple match rules like -> requiredDuringSchedulingIgnoredDuringExecution 

###########################################################################################################################################
##################################################### HELM ##############################################################################
##########################################################################################################################################
1.Which version of Helm you have used ?
Ans: v3.10.0 is used 

2.why do we use Helm ?
Ans: We are using helm as a package manager tool for Kubernetes. 
        We generally deploy all Kubernetes manifest files using helm only.
        I have written helm charts and we use to deploy microservices by installing the helm charts. 

3. how you use to deploy helm charts in kubernetes cluster ?
- we have written all the helm charts before hand only and we use to store those helm charts as a artifact on nexus server in helm repository type.
- we use to deploy them using jenkins.
- suppose we want to make some changes in chart. then we use to re update that charts or values.yml ( customized data) file and use that as a latest artifact to get deployed on EKS cluster.

3.explain me the folder structure of helm ?
Ans:  Helm chart structure is divided into 3 components.

        1. chart.yaml: It contains  metadata for helmchart. like general information about the helm charts.
               
              apiVersion: v2
              name: demochart
              description: A Helm chart for kubernetes
              type: application
              version: 0.1.0
              appVersion: 1.16.0

              we don't need to type all these information manually. Once we execute the command "helm create <chart_name>" it will create automatically.
              

        2. templates directory : This directory contains the all manifest files like deployement.yml service.yml,ingress controller files etc.
        3. values.yaml: - Its used to pass the vaiaibles value in manifest files.
                       - values.yaml is normal properties file where we store version number or basic configuration. So we don't have to update configuration in each and 
                         individual file. So values.yaml will be one place where you go and update or modify the configuration. So that helm can fetch those values and update 
                         it inside deployment.yaml and service account.yaml or any other yaml configuration.
4. what are the advantages of using helm 
Helm is easy to use, to install just use helm install chart name and it will install all the manifest of that microservice , then also easy to update beacuse we just need to update values.yml, then it is easy to rollback also, just use helm rollback and it will roll back the changes. 


4.commands used in helm ?
Ans: 

       1. helm create demochart: here "demochart" is chart name. After executing this command, we will get "demochart" directory. 
       2. tree demochart: it is used to verify the structure of helm chart "demochart".
       3. helm install <release_name> <chart_name>: it is used to install given chart name.
       4. helm list -a : it is used to verify helm install command.
       5. helm repo list : it is used to list the repositories.
       6. helm repo add <repository_name> <repository_url> : it is used to add the chart repository.
       7. helm plugin list: it is used to list the plugin.
       8. helm upgrade <release_name> <chart _name> : it is used to upgrade helm chart.
       9. helm diff revision myhelloworld 1 2 : it is used to make difference between revision 1 and 2	
      10. kubectl get deployment : it is used to give information about deployment.

5.how we can roll back any version using helm ?   
Ans: helm history <release-name> 
by using command "helm rollback <release_name> <revision>"

###########################################################################################################################################
################################################# linux ##################################################################################
#######################################################################################################################################

1. How to create Passwordless Authentication:
Step 1: Create Authentication SSH-Keygen Keys on Serverl
#ssh-keygen -t rsa

Step 2: Create .ssh Directory on Server2
#ssh root@10.10.10.11 mkdir -p .ssh

Step 3: Upload Generated Public Keys to Server2
#cat .ssh/id_rsa.pub | ssh root@10.10.10.11 'cat >> .ssh/authorized_keys'

Step 4: Set Permissions on Server2
#ssh root@10.10.10.11 "chmod 700 .ssh; chmod 640 .ssh/authorized_keys"

Step 5: Login from Serverl to Server2 Server without Password


2. explain file system in linux ?
/
├── bin - Essential user binaries (commands like ls, cat, cp)
├── boot - Bootloader and kernel files used during system startup
├── dev - Device files (like /dev/sda for disks, /dev/null)
├── etc - System configuration files
├── home - Home directories for normal users
├── lib - Shared libraries needed by system programs
├── root - Home directory of the root (superuser) 
├── tmp  - Temporary files (cleared on reboot)
├── usr  - User programs, libraries, documentation
└── var  - Variable data like logs (/var/log), mail, spool files

3. linux command to check which process or application are running in background ?  => ps -aux

4. command to check kernel version in linux ?   =>  uname -r

5. how to check which operating system (o.s) is used in linux ?  => cat /etc/os-release

6. explain booting process ?

BIOS - basic input output system executes MBR 

MBR - master boot records 

GRUB - GRAND unified bootloader executes kernel 

KERNEL - KERNEL executes /sbin/init 

INIT - init executes runlavel program 

RUNLAVEL - RUNLAVEL programs are executes from /ext/rc.d 


7. what is inode ?
to check inode of file use ls -i command 
Each file is identified by an inode — contains metadata like file size, owner, permissions, timestamps, and disk block locations.

8. what is use of netstat command ?
to check which application is running on which ports in our OS, 
we Use sudo ss -tulnp or sudo netstat -tulnp to see which ports are open and which services are bound to them.

9. uptime command ?  => uptime uptime → shows load averages (1, 5, 15 min).
uptime shows how long the system has been running and its load average.

10. how to check the instance / system logs ?
we can check the logs in var/log folder 

11. how to check the application logs like logs of apache ?
we can check the logs in var/log/apache

 
12. what are special characters in linux ?
“In Linux, special characters are symbols that have specific functions in the shell.
For example, * and ? are used as wildcards, > and < are used for input/output redirection, | is used for piping, and && or || are used for command chaining.
Quotation marks (', ", \) control how the shell interprets special characters, and symbols like $, ~, and / represent variables, home directories, and file paths.”

13. what does command "$?" means
symbols like $, ~, and / represent variables, home directories, and file paths.
  
14. what is mean by swap memory ? how to set it ?
Swap acts as a backup memory — when RAM is exhausted, inactive pages from RAM are moved (swapped) to disk to free up physical memory.
Swap memory is a portion of the disk used as virtual RAM when the system’s physical memory (RAM) is full. It allows processes to continue running, though slower, because disk access is slower than RAM access.

15. how to mount any volume with specific directory ?
You need to format it with a file system (usually ext4) 
sudo mkfs -t ext4 /dev/xvdf
Create a mount point directory
sudo mkdir /data
sudo mount /dev/xvdf /data


16. 5 commands to check the cpu utilization of server ?
top - Shows real-time system stats including CPU, memory, and processes.
htop - Enhanced version of top Color-coded, shows per-core CPU usage.
mpstat - P ALL 1 - Shows CPU utilization per core.
vmstat 1 - Shows CPU, memory, swap, and I/O statistics.
sar -u 1 5 - Shows CPU usage over time (-u = CPU stats, 1 5 = 1-second interval, 5 times).


17. what is mean by cronjob in linux  ? how to set it ? 
cron job is a scheduled task in Linux that runs automatically at specified intervals. 
commands to execute - 
crontab -e
0 2 * * * /home/user/backup.sh will execute this shell script everyday at 2 am and will take a backup 
crontab -l - list the running crontab 


18. what is the use of "nslookup" command ?
It’s a command-line utility used to query Domain Name System (DNS) to get information about IP addresses, domain names, or DNS records.

19. what is the use of ipconfig command ?
It’s a Windows command-line utility used to view and manage network configuration of your machine.

20. what are different filesystems in linux ? Filesystem	Use Case	Pros	Cons

ext4	Default Linux filesystem	Stable, journaling, supports large files, good performance	Limited advanced features like snapshots
XFS	High-performance, large files	Fast, scalable, supports journaling, good for big data	Not ideal for small files
Btrfs	Modern FS, snapshots, compression	Supports snapshots, compression, RAID, pooling	Still evolving; slightly complex
ext3	Older journaling FS	Stable, journaling	Slower than ext4, max volume smaller
ext2	Legacy, simple FS	Lightweight, simple	No journaling, higher corruption risk
tmpfs	Temporary RAM storage	Extremely fast, cleared on reboot	Volatile, limited by RAM size
proc	Kernel and process info	Virtual FS for system info	Not for persistent storage
sysfs	Hardware and device info	Exposes kernel/device info	Virtual; read-only for normal use
FAT32 / VFAT	USB drives, Windows compatibility	Cross-platform, simple	Max file size 4GB, no permissions
NTFS	Windows filesystem	Cross-platform support via ntfs-3g	Slower write performance on Linux
ZFS	Advanced storage pools	Snapshots, compression, high reliability	Complex, memory-hungry, not default on Linux

21. how to configure NFS ? 
Network file system
NFS allows a Linux system (client) to access directories and files located on another Linux system (server) over a network as if they were local.
“NFS (Network File System) allows a Linux client to mount directories from a Linux server over a network as if they were local.
To configure NFS, we install NFS packages, create a shared directory on the server, define exports in /etc/exports, start the NFS service, allow firewall access, and then mount the shared directory on the client.
Permanent mounting can be done via /etc/fstab.”
Useful for shared storage between multiple servers or VMs.

22. how to configure LVM in linux ?
Logical Volume Manager
LVM (Logical Volume Manager) is used in Linux for flexible disk management. It allows creating logical volumes from physical disks, resizing volumes dynamically, and taking snapshots.
The process involves creating Physical Volumes (PV), combining them into a Volume Group (VG), and then creating Logical Volumes (LV). After formatting, the LV can be mounted like a regular filesystem. LVM provides flexibility over traditional partitioning.”

23. what is zombie process in linux ? 

it is a process that has finished execution but its entry stil exits in the proces table because its parent process hasn't read its exit status yet. 
the process is dead but you can see the PID of that process. we can clean it by killing the parent process use kill -9 <parent PID> the zombies will get clean directly. 

24. how can we lock any user. 
passwd -<username> 

25. where can we set the password policy. 
sudo nano /etc/security/pwquality.conf 

26. our server is running very slow or it is in hang state, how can we troubleshoot 

use top command, it will show you the details of running process in our machine with the CPU and RAM utilization of thoes service, we will check which service is taking more RAM and CPU and then will discusse with the team and kill the processes which are not required. 
to kill process use kill -9 <PID>

will check the root volume which is  c drive of our service is full or not if it is full the we will move few files in s3 bucket. 

will delete the files from temp folder of our server so we can release some space. 

will check the IOPS of volume if IOPS are not ther it will hang, so we can increase the IOPS also. 

27. what will happen if we do useradd 

new user will get created. user details will be in CAT /etc/passwd
user folder will get created in /home folder 
new group will get created in /etc/group
new details will get added in /etc/shadow password will get stored in encrypted format.  

28. how to downgrade java version. 
apt downgrade <version number>

29. how to provide sudo permission to a user. 
use command -> visudo

30. how to findout something in linux server. 
we can use grep command to find overall, and to find from last 10 dyas files then use find -mtime -10, or to find anything less than 10mb then use -msize -10  

31. use of head and tail command 
cat filename | tail 5 -> to check last 5 command 
cat filename | head 10 -> to check the first 10 lines of file 

32. how can we print the last 3rd line of any file 

use command cat filename | tail 3 | head 1 

33. how to print in ascending order  --> use sort command 

34. words count in file --> wc -l 

35. how to check size of file. --> du -sh 

36. what is hardlink and softlink.

its a shortcut and hard link will not delete if the main file delets, it works as a backup file of your main file. and in softlink its just a short cut and If the original file is deleted, the soft link becomes broken.

37. how to increase volume in ec2
once we added or increase the volume will use command 
df -h 
growpart 

38. how to check block device 
lsblk 

39. how to check hiddne file 
ls -a 

40. what is the difference between chown and chmod 
chown will change the ownership of a file or directory 
chmod will change the permission which is read write and execute 

41. what is selinux  
A kernel-level security module that enforces mandatory access control (MAC) policies on Linux systems.
Adds an extra layer of security beyond standard file permissions. 

42. how can we create cronjob in linux 
cronjob is use to schedule any task in you linux system, sending logs to s3bucket or execution of any script on server at perticuler time.
there are 5 stars in cronjob which is minute, hour, day of month, month, day of week. 

to create cronjob you can use command crontab -e 
to list all the cronjob then use crontab -l 
to delete cronjob use crontab -r 

43. how to check who is logged in and what they are doing.
use command -> w 

44. how can we check the read write execute permissions of all the files. 

using command ls -ltr 
 
45. What details you can see in TOP command ?
System Summary (top section):
Uptime – How long the system has been running.
Load average – System load over the last 1, 5, and 15 minutes.
Tasks – Total, running, sleeping, stopped, and zombie processes.
CPU usage – Shows user, system, idle, and I/O wait time.
Memory usage – Total, used, free, and available RAM and swap memory. 

Process List (bottom section):
For each running process, it shows:
PID – Process ID
USER – Owner of the process
PR / NI – Priority and nice value
VIRT / RES / SHR – Virtual, resident, and shared memory usage
%CPU / %MEM – CPU and memory usage percentage
TIME+ – Total CPU time used
COMMAND – Process name or command

46. what do you mean by load average
Load average shows the average number of processes that are either running or waiting for CPU time over specific time intervals.

In Linux, you usually see three numbers in the load average (for example: 0.25, 0.40, 0.55) — they represent the system load over the last 1, 5, and 15 minutes respectively.

47. what is the command to check running process 
Commonly used commands to check running processes in Ubuntu are ps -ef, top, htop, and pgrep pstree.

48. what is the use of resolve.conf file. 
The /etc/resolv.conf file in Linux is used to configure DNS resolution — it tells the system which DNS servers to query when resolving domain names into IP addresses.


**SHELL SCRIPTING**
what is mean by shebang ? (#!/bin/bash)
set is the use of "set -xv" command in shell script ?
what is the use of "$?" command in linux ?
file system in linux ?
for loop ?
if else ?
how to set variables in linux ?
Passwordless authentication ?
use of sed command ?


**imp commands**
awk
tail
head
grep
find
du -sh
df -hT
cut
cat
chown
chmod
traceroute
ping
dig
top
ipconfig
w
export 
$?
wget
tar
curl - to get response of any URL
################################################################################################################################################
############################################ Jenkins #########################################################################################
#############################################################################################################################################

1. So have you worked on gitlab or any other CI-CD tool ?

No, I have only worked only on jenkins

2.  What is the difference between CI and CD

CI = integrate and test code continuously.
CD = deliver/deploy that tested code automatically to users.

3. Difference between continuous delivery and contineous deployment?

Continuous delivery - Code is automatically built, tested, and prepared for release, but deployment to production requires manual approval.
Continuous deployment - Code is automatically built, tested, and deployed to production without manual intervention.

4. what kind of structure you follow for writing the code in your company ?
we have used shared library stucture 


5. What is the folder structure of any share library ?
Shared libraries in Jenkins are used to reuse common pipeline code across multiple projects.
Folder	Purpose
vars/	Contains global pipeline functions (accessible directly in Jenkinsfile as steps). Each .groovy file defines one global variable or method.
src/	Contains reusable classes in package structure (like normal Groovy/Java classes). Used for logic-heavy code.
resources/	Holds non-Groovy files (JSON, YAML, templates) accessed using libraryResource().
test/	Contains unit tests for library functions.
README.md	Documentation for developers.

5. Which type of pipelines you have written in your company ?
we have created freestyle and pipeline 

6. What kind of architecture you follow for your jenkins setup ?
we have created masteer and worker node setup. 

7. In which language your applications code is written ?
java

7. What is pom.xml file ?
It defines the configuration and structure of a Maven project. Maven reads this file to build, test, and manage project dependencies.

8. What is the full form of POM ?
Full form of POM: Project Object Model

6. How you will integrate github with jenkins ?
Install required plugin in Jenkins, Create a Jenkins job, Configure GitHub Webhook, also we need to Set build trigger in Jenkins using github webhook. 

7. How you will integrate sonar qube with jenkins ?
Install SonarQube Scanner Plugin, Configure SonarQube Server in Jenkins, Add SonarQube Scanner Tool, Create or Update Jenkins Pipeline. 

8. How you will integrate nexus with jenkins ?
Install required plugins in Jenkins which is Artifact Uploader Plugin, Configure Nexus credentials in Jenkins, Set up Maven settings (if using Maven) 

9. Tell me which plugins you have used in your company?
Git Plugin – To pull source code from GitHub or GitLab repositories.
Blue Ocean plugin - used to provide a modern, visual, and user-friendly interface
Maven Integration Plugin – For building Java-based projects using Maven.
Docker Plugin – To build, push, and run Docker images within Jenkins jobs.
Kubernetes Plugin – To deploy builds directly to Kubernetes clusters.
SonarQube Scanner Plugin – For static code analysis and quality checks.
Email Extension Plugin – To send build notifications to developers.
Credentials Binding Plugin – For securely managing credentials like AWS keys or tokens.
Slack Notification Plugin – To send build status alerts to Slack channels.
JUnit Plugin – For publishing and visualizing test results.
Artifact Uploader Plugin – To move build artifacts to servers.
Pipeline Plugin – To define and run Jenkins pipelines using Jenkinsfile.

10. Use of blueocean plugin ?
The Blue Ocean plugin is used to provide a modern, visual, and user-friendly interface for Jenkins pipelines.

11. Which is the home directory of jenkins ?
JENKINS_HOME = /var/lib/jenkins

13. How you make sure you have backup of jenkins home directory with you ?
You back up the $JENKINS_HOME directory regularly because it stores all Jenkins configurations, jobs, plugins, and credentials.
0 2 * * * tar -czf /backup/jenkins_home_$(date +\%F).tar.gz /var/lib/jenkins

14. So which code build tool you have used in your company ?
maven 

16. Explain me any one end to end CI-CD pipeline you have worked on recetly ?
Git checkout → Maven build → Unit test → SonarQube → Docker build → Trivy scan → Push to ECR → Assume role → Deploy to EKS

17. what is the pipeline structure you have in your complany, 
we have 4 aws account for our project, so 1 accout for jenkins pipeline, then 2nd account for QA, 3rd account for staging, and 4th account for the production, and to to this we haev done VPC peering of all the VPC, then we have used assume role and trust policy architecture to connect from jenkins server to all the servers, 
it workes like - we have created (another aws account role), in all the QA, staging, and production aws account, because of this trust relationship get created with my jenkins servers, and also created IAM role which is assume role in my jenkisn server and attached that role to all the jenkins server so now all my jenkins server can do the changes in another aws account. 

18. suppose i want to trigger one mail or want to execute any command after the build stage is done in what case what you will use 
so we can use post in jenkins in that post we can mention success of failure as condition and the action that needs to be perforemd after success of failure 


19. can we  install multiple maven or java versions on jenkins ?
Yes, we can install and use multiple versions of Maven or Java in Jenkins.
How it works:
Jenkins allows you to manage different tool versions under “Global Tool Configuration”.
You can add multiple versions of JDK and Maven, give each a unique name, and Jenkins will download or use the specified path.
In your pipeline or freestyle job, you can select which version to use.

20. how to install Jenkins plugin offline without internet ? 
In Jenkins UI, go to:
Manage Jenkins → Plugins → Advanced → Upload Plugin
Choose the .hpi or .jpi file and click Upload.
or 
copy manually from any another server to the jenkins server on war/lib/jenkins/plugins folder and restart the jenkins. 

21. difference between freestyle ,maven,scripted and declarative pipelines ?
 Freestyle Project - Oldest and simplest job type in Jenkins. GUI-based configuration — no coding required.
 Maven Project - Specifically designed for Java projects using Maven. jenkins automatically detects the pom.xml file and runs Maven goals.
 Provides built-in reporting for test results and build artifacts.
 Scripted Pipeline - Older, Groovy-based syntax for defining Jenkins pipelines as code (Jenkinsfile). Very flexible but verbose and less readable. Logic-heavy (supports loops, conditions, error handling). 
 Declarative Pipeline - Modern and structured syntax for pipelines. Easier to read, write, and maintain. Enforces predefined sections like agent, stages, and steps. Preferred for most production use cases. 

22. can we change the default port number of jenkins application and if yes then how ? 
Yes, we can change the default port number of the Jenkins application. sudo vi /etc/default/jenkins 

23. can we change the default home directory of jenkins ?
create new directory and change the owner of that file to jenkins using chown command 
sudo vi /etc/default/jenkins 
JENKINS_HOME=/new_jenkins_home
sudo systemctl start jenkins

24. how we can secure the jenkins pipeline ? any security plugins ? 
Role-Based Authorization Strategy Plugin and Audit Trail 
Enable Authentication & Authorization 

25. suppose i have 2 pipelins. A & B. now i want that the pipeline B should run automatically once pipeline A has ran successfully. how you will do it ? 
Using “Build Trigger” in Pipeline B
In Pipeline B, go to:
Configure → Build Triggers → Build after other projects are built
Enter Pipeline_A as the upstream project.
This creates an upstream/downstream relationship automatically.

26. do you know the concept of parameters in jenkins ? diff parameters ? 
Yes, in Jenkins, parameters are used to make a job or pipeline dynamic and reusable by accepting user input before the build starts. Parameters let you control build behavior without changing the pipeline code. 
String Parameter - Accepts a simple text value (e.g., version, environment name). 
Boolean Parameter - Provides a checkbox for true/false input. 
Choice Parameter - Dropdown menu to select one option from predefined choices. 
Password Parameter - Securely accepts sensitive data (hidden input). 
File Parameter - Allows uploading a file to be used in the build. 
Run Parameter - Lets you trigger the current job based on a specific build of another job. 

27. why we do master slave setup of jenkins ?
To distribute build workloads and improve performance.

28. how you will send notification on mail once your pipeline has run successfully or failed ?
By configuring email notifications in Jenkins using the Email Extension Plugin (also known as Email-ext).
configuring the SMTP service. 

29. what is the use of thin backup plugin ? 
The ThinBackup plugin in Jenkins is used to back up and restore Jenkins configuration data.

30. explain me all the jenkins pipeline that you have used in your project. 

the first pipeline is terraform pipeline, which is being used by all the devops engineer in our group, where we have comman git repo where we write the terraform templates and merege the changes with master branch, once approved the pipeline will get triggered and it run the commands like terraform init, terraform validate, terraform plan, and terraform apply. and the infra will get ready. 

then the second pipeline is for UI of our project, where frontend developer will push the code and after merging the changes with master branch, developer manually trigger the pipeline, with choice parameters, like QA, stage, or PROD. 
accordingly pipeline will start running and first stage is to pull the changes from github and then, thne build the code, if it is .net code then .net build will run and then the test cases will run and get stored in one txt file then the code will go for the sonarqube analysis then it will pass throught all the quality gate, then based on the env that you have chose it will take the assume role of that accout and push the changes on the S3 bucket. 

the flow is like ==> git pull -> build stage -> unit tets -> sonarqube analysis -> choes the assume role -> deploy on the s3 bucket 


then we have pipeline for the backend microservice, first the developer will do the changes in the code then it will get merged with the master branch, once done developer will trigger the pipeline with the choice parameter like QA stage or prod,and will provide the string parameter with the jira ticket number. based on that pipeline will start executing, the first stage is the git pull command, then the unit test will strat running, then sonarqube analyses will start executing once completed the java arifact will pushed to the nexus then the docker build process will execute then the created image will get scaned by the trivy, once it passes all the checkpoint based on the assume role it will update the helm chart with the new image tag and do the helm upgrade, once done the changes will get reflected. 

Maven build -> Unit tests -> SonarQube analysis -> Upload artifact to Nexus -> Docker build (pull artifact from Nexus)
-> Trivy image scan ->  Push Docker image to ECR -> Assume role (AWS IAM) -> Deploy to EKS using Helm


#####################################################################################################################################
####################################################### Maven ###########################################################################
########################################################################################################################################

MAVEN :

1.	Explain Maven lifecycle ?
Key Phases:

validate – checks project structure and configuration
compile – compiles source code
test – runs unit tests
package – packages compiled code into a JAR/WAR
verify – runs checks to ensure package is valid
install – installs package into local Maven repository (~/.m2/repository)
deploy – copies final artifact to remote repository

2.	Explain folder structure of maven ?
project-root/
│
├── pom.xml                # Main Maven configuration file
│
├── src/
│   ├── main/              # Main application code
│   │   ├── java/          # Java source files
│   │   ├── resources/     # Configuration files, property files
│   │   └── webapp/        # Web application files (for WAR projects)
│   │       ├── WEB-INF/
│   │       └── static/
│   │
│   └── test/              # Test-related code
│       ├── java/          # Unit test Java classes
│       └── resources/     # Test configuration files
│
├── target/                # Generated output (compiled classes, JAR/WAR)
│
└── README.md              # Optional documentation


3.	What are different maven goals ?
Maven goals are specific tasks that perform actions within a phase or directly when invoked. Each plugin defines one or more goals.
Goal	Plugin	Description
clean:clean -->	maven-clean-plugin	Deletes the target/ directory
compiler:compile --> 	maven-compiler-plugin	Compiles main source code
compiler:testCompile	--> maven-compiler-plugin	Compiles test source code
A goal is the smallest executable unit in Maven.
Goals can be executed independently or as part of a phase.
Most goals come from Maven plugins (e.g., compiler, surefire, deploy).



4.	What is the dependency of maven ? 
-	we should have java installed in server 

5.	Why / for what purpose you have used maven in your organization?
-	In our project, the programming language used to build the application was java. So to build the package ( JAR /WAR files ). We were using maven and For building the packages we used different maven goals.

6.	What is full form of .jar , .war ?
.jar → Java ARchive
.war → Web Application ARchive

7.	What does pom.xml file contains ?
The pom.xml (Project Object Model) file is the core configuration file in Maven. It defines the project details, dependencies, plugins, and build settings.


############################################################################################################################################################################################################# NEXUS ############################################################################
############################################################################################################################################
1.	What is mean by Artifacts ?
-	Artifacts means the final product which is ready to be used as and when required.for example : docker image , helm charts , .war / .jar packages

2.	What is nexus  ? 
Or
3.	In your company where you have used the nexus tool ? 
-	In our company we used nexus as a artifact server. We use to store all our final artifacts like docker image , helm charts and jar/war packages on nexus.
-	So we used to store different artifacts in different repositories like
 Docker – docker repository type
Helm – helm repository type
Java artifacts – maven repository type.

4.	How to integrate Nexus with Jenkins ?
integrate Nexus with Jenkins by configuring Maven’s settings.xml with Nexus credentials, adding the repository URLs in the project’s pom.xml, and using the mvn deploy goal in Jenkins to upload build artifacts to Nexus.

5.	How to create users in nexus ?
Login to Nexus, Go to Security → Users, Click “Create local user”, Fill in details, Assign Roles, Click “Create user"

6.	What are the different repositories type in nexus ?

Docker – docker repository type
Helm – helm repository type
Java artifacts – maven repository type.


7.	Default port number for nexus ?
port 8081

8.	How to login into Nexus ? how to push and pull docker images from nexus ?
-	You need to use same commands which were used to pull / push / login into docker hub account.
-	Docker login
-	Docker push
-	Docker pull

################################################################################################################################################################################################### SONARQUBE :##########################################################
############################################################################################################################################
1.What is the use of sonarqube ?
SonarQube is a tool used for continuous inspection of code quality. It analyzes source code to detect bugs, vulnerabilities, code smells, and duplications.

2.Why you have used sonarqube in your company ?
- We use Sonarqube for static code analysis . we have set quality gates which help us to check the code status.

3.How to integrate sonarqube with Jenkins  ?
Integrate SonarQube with Jenkins by installing the SonarQube plugin, configuring the SonarQube server and authentication token in Jenkins, and then adding a SonarQube analysis step (e.g., mvn sonar:sonar) in the Jenkins pipeline or job. 

4.How to create token of any user in sonarqube ?
Log in to SonarQube with that user’s account.Go to My Account → Security → Generate Tokens. Enter a token name and click Generate.Copy the generated token — it’s shown only once.

5.once you install sonarqube , explain the folder structure of sonarqube folder ?
bin/ → start/stop scripts

conf/ → configuration files

extensions/ → plugins

logs/ → log files

data/, temp/ → runtime data

web/ → web interface files

6.what do you mean by quality gates and quality profiles ?
1. Quality Gates

Meaning:
A Quality Gate is a set of conditions that determine whether the code passes or fails quality standards — like a checkpoint in your CI/CD pipeline.
Example conditions:
Code coverage > 80%
No new critical or blocker bugs
Duplicated code < 3%
Maintainability rating = A

Quality profile. 
Meaning:
A Quality Profile defines which rules are used for analyzing code for a given language. 
For Java → “Sonar Way” profile might include 500+ rules.
You can create a custom profile enabling/disabling specific rules to match your organization’s standards

7.what kind of quality gates you had defined for your code analysis ?
Code Coverage ≥ 80% on new code
No Blocker or Critical Issues
Duplicated Lines Density ≤ 3%
Maintainability Rating = A
Security Rating = A
Reliability Rating = A
No New Code Smells

################################################################################################################################################################################################  TOMCAT #######################################################################################
###########################################################################################################################################
1.Which web server you used for deploying your java code ?
Tomcat 

2.How to create users in tomcat ?
Create users in Tomcat by editing the tomcat-users.xml file (in conf/ directory) and adding user entries with roles, then restart Tomcat to apply changes.

3.What is use of  catalina file ?
The catalina script (catalina.sh or catalina.bat) is used to start, stop, or configure the Apache Tomcat server from the command line.

4.Where can we check the logs of tomcat application ?
Tomcat application logs are stored in the logs/ directory inside the Tomcat installation path, typically:$CATALINA_HOME/logs/
 

5.Explain the folder structure of tomcat ?

apache-tomcat/
├── bin/                # Startup and shutdown scripts (catalina.sh, startup.sh, shutdown.sh)
├── conf/               # Configuration files (server.xml, web.xml, tomcat-users.xml)
├── lib/                # Core JAR files required for Tomcat to run
├── logs/               # Log files (catalina.out, localhost.log, etc.)
├── webapps/            # Deployed web applications (.war or exploded folders)
├── work/               # Temporary files and compiled JSPs
├── temp/               # Temporary storage used by Tomcat during runtime
└── README, LICENSE     # Documentation files


